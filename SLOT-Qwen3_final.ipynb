{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SLOT方法实现--基于Qwen模型\n",
    "\n",
    "### SLOT方法的核心思路是：\n",
    "- 对任意输入的 prompt，只使用原始模型进行前向推理；\n",
    "- 在不改动模型权重的前提下，仅对生成过程中的隐藏状态（如最后一层 hidden state）进行扰动；\n",
    "- 引入一个可训练的小型参数 `delta`，其形状为 `[1, 1, hidden_size]`；\n",
    "- `delta` 是在 prompt 自身的预测任务上训练得到的，其目标是提升模型对下一个 token 的预测能力；\n",
    "- 训练完成后，在生成过程中将该 `delta` 加到隐藏表示上，进而影响输出 token，达到引导生成的目的。\n",
    "\n",
    "\n",
    "### 该方法具备如下优点：\n",
    "- **无需微调大模型参数，成本低**\n",
    "- **适用于任意任务 prompt，无需重构模型结构**\n",
    "- **可以灵活插拔，引导模型按预期生成**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19dd3fbc873e2017"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4b566540e8f0a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:03.790718200Z",
     "start_time": "2025-06-05T16:01:01.005297300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载Qwen-0.6B模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f7ec2640c35c5e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = r\"D:\\pythonProject\\DeepSeek\\Recsys\\AnimeLLMRec\\Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.539253200Z",
     "start_time": "2025-06-05T16:01:03.789717900Z"
    }
   },
   "id": "43b0e0d567533bf6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 输出最后一层 H state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d020c4f88a2a56d3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cc177fbbcb27c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.732402700Z",
     "start_time": "2025-06-05T16:01:06.542888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 5, 1024])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 输入 prompt ===\n",
    "prompt = \"请你详细介绍一下西湖。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# === 前向传播，输出包含 hidden_states ===\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "# === 获取倒数第一层的 Hidden States H（transformer 最后一层输出）===\n",
    "# shape: [1, seq_len, hidden_size]\n",
    "H = outputs.hidden_states[-1]\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 查看Token的切割"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d85d8f1783672d7b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14d91faf88e7f4f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.767400900Z",
     "start_time": "2025-06-05T16:01:06.731401200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Index  Token ID Token\n0      0     56568     你\n1      1    109182   是一位\n2      2    104715   专业的\n3      3      8903     日\n4      4     77128     志",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>Token ID</th>\n      <th>Token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>56568</td>\n      <td>你</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>109182</td>\n      <td>是一位</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>104715</td>\n      <td>专业的</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8903</td>\n      <td>日</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>77128</td>\n      <td>志</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import get_token_alignment_df\n",
    "\n",
    "prompt = \"\"\"你是一位专业的日志分析专家，请基于以下日志条目，提取其中的异常模式、潜在根因，并提出清晰的修复建议。\n",
    "请注意以下要求：\n",
    "- **必须**使用人的自然语言习惯进行简洁清晰的表达，避免冗长术语；\n",
    "- **每一个异常判断必须引用原始日志内容**，说明异常来源；\n",
    "- 分析结果应**按逻辑分段列出**，便于阅读与排查。\n",
    "以下是异常日志列表：\n",
    "- 2025-04-23 14:42:36.060 [INFO] database P0000010414\n",
    "请你进行分析。\n",
    "\"\"\"\n",
    "\n",
    "get_token_alignment_df(prompt=prompt, tokenizer=tokenizer).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 输入 Prompt，得到W_vocab 和 H state矩阵"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58966fd8920c59b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5bc04edf14d2b3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.790401300Z",
     "start_time": "2025-06-05T16:01:06.748401600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([151936, 1024])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 输入 prompt ===\n",
    "prompt = \"请你详细介绍一下西湖。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"][0]  # 去掉 batch 维度\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# === 前向传播，输出包含 hidden_states ===\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "# === 获取倒数第一层的 Hidden States H（transformer 最后一层输出）===\n",
    "# shape: [1, seq_len, hidden_size]\n",
    "H = outputs.hidden_states[-1]\n",
    "# === 获取 W_vocab 权重矩阵 ===\n",
    "# 对于 GPT 类模型，输出 logits 是 H @ W_vocab.T\n",
    "\n",
    "# === 获取 vocab 权重矩阵 ===\n",
    "W_vocab = model.lm_head.weight  # shape: [vocab_size, hidden_size]\n",
    "W_vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt自我预测下一个token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99d9624c5c3c99e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 这是字符 '请你' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    用          11622      0.071626\n",
      "2    设计         70500      0.063536\n",
      "3    帮我         108965     0.062522\n",
      "\n",
      "🔍 这是字符 '详细' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    解释         104136     0.276219\n",
      "2    分析         101042     0.081168\n",
      "3    地          29490      0.069583\n",
      "\n",
      "🔍 这是字符 '介绍一下' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    “          2073       0.019732\n",
      "2    《          26940      0.013023\n",
      "3    如何         100007     0.012535\n",
      "\n",
      "🔍 这是字符 '西湖' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    大学         99562      0.076652\n",
      "2    的          9370       0.074637\n",
      "3    龙          99465      0.052748\n",
      "\n",
      "🔍 这是字符 '。' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    �          8908       0.402138\n",
      "2               220        0.124658\n",
      "3    �          6567       0.041067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_k = 5\n",
    "\n",
    "\n",
    "def get_top_k_predict(prompt, H_state, model, tokenizer, top_k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"][0]  # 去掉 batch 维度\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    W_vocab = model.lm_head.weight\n",
    "\n",
    "    predict_next_tokens = []\n",
    "    next_tokens = []\n",
    "    for i, (token_id, token, h_vec) in enumerate(zip(input_ids.tolist(), tokens, H_state[0])):\n",
    "        logits_i = torch.matmul(h_vec, W_vocab.T)  # [vocab_size]\n",
    "        probs_i = torch.softmax(logits_i, dim=-1)\n",
    "        topk = torch.topk(probs_i, k=top_k)\n",
    "\n",
    "        top_ids = topk.indices.tolist()\n",
    "        top_probs = topk.values.tolist()\n",
    "\n",
    "        # ✅ 用 decode 解决乱码问题\n",
    "        top_tokens = [tokenizer.decode([id]).replace(\" \", \"\") for id in top_ids]\n",
    "\n",
    "        char = tokenizer.decode(token_id)\n",
    "\n",
    "        for j in range(len(topk[0])):\n",
    "            next_tokens.append([i, char, top_tokens[j], top_ids[j], top_probs[j]])\n",
    "    return pd.DataFrame(next_tokens, columns=['token_idx', 'char', 'char_predict', 'token_id', 'prob'])\n",
    "\n",
    "\n",
    "def pretty_print_top_k(df):\n",
    "    \"\"\"\n",
    "    漂亮地打印 DataFrame 中所有 token 的 top-k 预测结果。\n",
    "\n",
    "    参数:\n",
    "    - df: DataFrame，包含 get_top_k_predict 的输出\n",
    "    \"\"\"\n",
    "    token_indices = df['token_idx'].unique()\n",
    "    for token_idx in token_indices:\n",
    "        sub_df = df[df['token_idx'] == token_idx]\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        char = sub_df.iloc[0]['char']\n",
    "        print(f\"\\n🔍 这是字符 '{char}' 的下一个 token 预测：\")\n",
    "        print(f\"{'序号':<4} {'预测字符':<10} {'token_id':<10} {'概率':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, row in enumerate(sub_df.itertuples(), 1):\n",
    "            print(f\"{i:<4} {row.char_predict:<10} {row.token_id:<10} {row.prob:<.6f}\")\n",
    "\n",
    "\n",
    "top_k = 3\n",
    "top_k_df = get_top_k_predict(prompt=prompt, model=model, tokenizer=tokenizer, H_state=H, top_k=top_k)\n",
    "pretty_print_top_k(top_k_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.876400400Z",
     "start_time": "2025-06-05T16:01:06.778400900Z"
    }
   },
   "id": "4936be57206f49f1",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 通过 Prompt 自监督训练扰动向量 delta\n",
    "\n",
    "我们将使用 prompt 自身的 token 序列进行训练，优化一个可学习的扰动向量 delta，使模型更准确地预测下一个 token。\n",
    "\n",
    "- 输入 prompt: `[token_1, token_2, ..., token_n]`\n",
    "- 原理：通过最小化预测 token[i+1] 的损失，引导模型在每个位置的预测更接近实际下一个 token。\n",
    "\n",
    "对齐关系如下：\n",
    "logits[0] → token[1]\n",
    "logits[1] → token[2]\n",
    "logits[2] → token[3]\n",
    "...\n",
    "logits[n-2] → token[n-1]\n",
    "logits[n-1] → token[n]\n",
    "\n",
    "\n",
    "训练目标是使模型输出的每个 `logits[i]` 更靠近其对应位置的目标 token `token[i+1]`。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a94c414a21628ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# === 输入 prompt ===\n",
    "prompt = \"请你详细介绍一下西湖。\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:06.877400400Z",
     "start_time": "2025-06-05T16:01:06.826402300Z"
    }
   },
   "id": "34089cebc8f7651e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training delta: 100%|██████████| 3/3 [00:00<00:00, 42.25it/s]\n",
      "Training delta: 100%|██████████| 10/10 [00:00<00:00, 169.49it/s]\n",
      "Training delta: 100%|██████████| 30/30 [00:00<00:00, 192.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_delta_from_H(model, tokenizer, prompt, H_state, step=3, lr=1e-2):\n",
    "    \"\"\"\n",
    "    针对给定 prompt，通过优化隐藏状态 H 的扰动 delta，使模型更好地预测下一个 token。\n",
    "\n",
    "    参数：\n",
    "    - model: 语言模型（需具备 lm_head）\n",
    "    - tokenizer: 分词器\n",
    "    - prompt: 输入的文本 prompt（str）\n",
    "    - step: 优化步数（默认 3）\n",
    "    - lr: 学习率（默认 1e-2）\n",
    "\n",
    "    返回：\n",
    "    - delta: 训练得到的扰动向量，shape = [1, 1, hidden_size]\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: 编码 Prompt，并转为模型输入 ===\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # shape: [1, seq_len]\n",
    "\n",
    "    # === Step 3: 构建目标 token 对（预测下一个 token）===\n",
    "    # 当前 token 用于生成，目标 token 用于监督\n",
    "    current_ids = input_ids[:, :-1]  # shape: [1, seq_len-1]\n",
    "    target_ids = input_ids[:, 1:]  # shape: [1, seq_len-1]\n",
    "\n",
    "    # === Step 4: 初始化 delta（可训练参数）===\n",
    "    hidden_size = H_state.size(-1)\n",
    "    delta = nn.Parameter(torch.zeros((1, 1, hidden_size), device=H_state.device, requires_grad=True))\n",
    "\n",
    "    # === Step 5: 设置优化器与损失函数 ===\n",
    "    optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_log = []\n",
    "\n",
    "    # === Step 6: 开始优化 delta 参数 ===\n",
    "    for i in tqdm(range(step), desc=\"Training delta\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 扩展 delta 至每个 token 的位置（broadcast）\n",
    "        delta_broadcast = delta.expand(H_state[:, :-1, :].shape)  # shape: [1, seq_len-1, hidden_size]\n",
    "\n",
    "        # 对隐藏状态添加扰动\n",
    "        adjusted_H = H_state[:, :-1, :] + delta_broadcast\n",
    "\n",
    "        # 计算 vocab 维度的 logits（模拟 lm_head）\n",
    "        logits = torch.matmul(adjusted_H, model.lm_head.weight.T)  # shape: [1, seq_len-1, vocab_size]\n",
    "\n",
    "        # reshape 为 flat 形式用于计算 loss\n",
    "        logits_flat = logits.view(-1, logits.size(-1))  # shape: [token数, vocab_size]\n",
    "        targets_flat = target_ids.view(-1)  # shape: [token数]\n",
    "\n",
    "        loss = loss_fn(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "        # print(f\"step_{i}_loss: {loss.item():.6f}\")\n",
    "\n",
    "    return delta\n",
    "\n",
    "\n",
    "# 执行训练\n",
    "delta = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=3)\n",
    "\n",
    "delta_10 = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=10)\n",
    "\n",
    "delta_30 = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=30)\n",
    "# 保存 delta（如需）\n",
    "torch.save(delta, \"delta.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.156401200Z",
     "start_time": "2025-06-05T16:01:06.847401700Z"
    }
   },
   "id": "7a5d2b5caa792080",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 检查 delta 对 Prompt 自预测概率的提升效果\n",
    "\n",
    "我们将对比加不加 delta 时，模型对自身 Prompt 的 token 序列的 next-token 预测概率，观察是否出现更高的置信度或更集中的 top-k 输出。\n",
    "\n",
    "### 步骤如下：\n",
    "\n",
    "1. 使用 `get_top_k_predict()` 函数，对 prompt 的每个 token，预测它下一个 token 的概率分布。\n",
    "2. 输出每个位置 top-k（例如 top-3）预测的 token 及其概率。\n",
    "3. 通过与加 delta 后的预测结果对比，评估 delta 是否提升了正确 token 的排名或概率。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e3bf85f738c846"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 这是字符 '请你' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    用          11622      0.071626\n",
      "2    设计         70500      0.063536\n",
      "3    帮我         108965     0.062522\n",
      "\n",
      "🔍 这是字符 '详细' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    解释         104136     0.276219\n",
      "2    分析         101042     0.081168\n",
      "3    地          29490      0.069583\n",
      "\n",
      "🔍 这是字符 '介绍一下' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    “          2073       0.019732\n",
      "2    《          26940      0.013023\n",
      "3    如何         100007     0.012535\n",
      "\n",
      "🔍 这是字符 '西湖' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    大学         99562      0.076652\n",
      "2    的          9370       0.074637\n",
      "3    龙          99465      0.052748\n",
      "\n",
      "🔍 这是字符 '。' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    �          8908       0.402138\n",
      "2               220        0.124658\n",
      "3    �          6567       0.041067\n"
     ]
    }
   ],
   "source": [
    "top_k_df = get_top_k_predict(prompt=prompt, model=model, tokenizer=tokenizer, H_state=H, top_k=3)\n",
    "pretty_print_top_k(top_k_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.175961700Z",
     "start_time": "2025-06-05T16:01:07.157401200Z"
    }
   },
   "id": "5111ffe92b8fb450",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 这是字符 '请你' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    用          11622      0.068048\n",
      "2    帮我         108965     0.064675\n",
      "3    设计         70500      0.060419\n",
      "\n",
      "🔍 这是字符 '详细' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    解释         104136     0.264910\n",
      "2    分析         101042     0.075175\n",
      "3    介绍一下       109432     0.070362\n",
      "\n",
      "🔍 这是字符 '介绍一下' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    “          2073       0.019204\n",
      "2    《          26940      0.013614\n",
      "3    如何         100007     0.012911\n",
      "\n",
      "🔍 这是字符 '西湖' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    大学         99562      0.075676\n",
      "2    的          9370       0.070524\n",
      "3    龙          99465      0.050014\n",
      "\n",
      "🔍 这是字符 '。' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    �          8908       0.396332\n",
      "2               220        0.115681\n",
      "3    西湖         110192     0.057694\n"
     ]
    }
   ],
   "source": [
    "top_k_df = get_top_k_predict(prompt=prompt, model=model, tokenizer=tokenizer, H_state=H + delta, top_k=3)\n",
    "pretty_print_top_k(top_k_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.188962500Z",
     "start_time": "2025-06-05T16:01:07.173962400Z"
    }
   },
   "id": "461dcc5a402f92e9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 这是字符 '请你' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    详细         100700     0.082929\n",
      "2    帮我         108965     0.066765\n",
      "3    用          11622      0.059523\n",
      "\n",
      "🔍 这是字符 '详细' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    解释         104136     0.230962\n",
      "2    介绍一下       109432     0.155141\n",
      "3    分析         101042     0.060067\n",
      "\n",
      "🔍 这是字符 '介绍一下' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    “          2073       0.018255\n",
      "2    《          26940      0.015142\n",
      "3    什么是        106582     0.014113\n",
      "\n",
      "🔍 这是字符 '西湖' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    大学         99562      0.072181\n",
      "2    的          9370       0.062687\n",
      "3    龙          99465      0.043884\n",
      "\n",
      "🔍 这是字符 '。' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    �          8908       0.368605\n",
      "2    西湖         110192     0.132821\n",
      "3               220        0.093241\n"
     ]
    }
   ],
   "source": [
    "top_k_df = get_top_k_predict(prompt=prompt, model=model, tokenizer=tokenizer, H_state=H + delta_10, top_k=3)\n",
    "pretty_print_top_k(top_k_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.219963500Z",
     "start_time": "2025-06-05T16:01:07.188962500Z"
    }
   },
   "id": "38adf5b0fdda2ea1",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 这是字符 '请你' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    详细         100700     0.383109\n",
      "2    帮我         108965     0.042952\n",
      "3    用          11622      0.033371\n",
      "\n",
      "🔍 这是字符 '详细' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    介绍一下       109432     0.512587\n",
      "2    解释         104136     0.123203\n",
      "3    描述         53481      0.043399\n",
      "\n",
      "🔍 这是字符 '介绍一下' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    《          26940      0.020553\n",
      "2    ，          3837       0.019420\n",
      "3    什么是        106582     0.016968\n",
      "\n",
      "🔍 这是字符 '西湖' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    大学         99562      0.051422\n",
      "2    的          9370       0.049989\n",
      "3    十          94498      0.038207\n",
      "\n",
      "🔍 这是字符 '。' 的下一个 token 预测：\n",
      "序号   预测字符       token_id   概率        \n",
      "----------------------------------------\n",
      "1    西湖         110192     0.691022\n",
      "2    �          8908       0.132904\n",
      "3               220        0.026918\n"
     ]
    }
   ],
   "source": [
    "top_k_df = get_top_k_predict(prompt=prompt, model=model, tokenizer=tokenizer, H_state=H + delta_30, top_k=3)\n",
    "pretty_print_top_k(top_k_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.253961600Z",
     "start_time": "2025-06-05T16:01:07.205961500Z"
    }
   },
   "id": "b915e0e316fc8905",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 用训练好的delta加在H上生成答案\n",
    "训练得到的 `delta` 是一个扰动向量，形状为 `[1, 1, hidden_size]`，它可以被加在隐藏状态 H 的最后一个位置上，调整模型对下一个 token 的预测方向。\n",
    "\n",
    "整个生成过程如下：\n",
    "\n",
    "1. 输入 prompt，得到 token 编码与初始隐藏状态 H。\n",
    "2. 对每一步生成：\n",
    "   - 前向传播，获取当前序列的 hidden states `H_cur`\n",
    "   - 取出最后一个位置的 hidden vector：`H_cur[:, -1, :]`\n",
    "   - 加上训练好的 `delta`：`H_adj = H_cur[:, -1, :] + delta.squeeze(1)`\n",
    "   - 使用模型输出层（lm_head）计算 logits：`logits = H_adj @ W_vocab.T`\n",
    "   - 用 `argmax` 或采样策略选择下一个 token\n",
    "   - 拼接进序列，进入下一步\n",
    "3. 最终生成若干 token，得到 delta 引导下的完整答案。\n",
    "\n",
    "此过程保持 prompt 不变，仅在 hidden space 中轻微调整，通常能对输出风格或关注内容起显著引导作用。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2de17b79870446c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_by_H(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    基于隐藏状态 H 添加扰动 delta 的方式进行文本生成。\n",
    "    \n",
    "    参数：\n",
    "    - model: LLM 模型\n",
    "    - prompt: 输入提示词\n",
    "    - tokenizer: 分词器\n",
    "    - delta: 扰动张量，shape=[1, 1, hidden_size]\n",
    "    - answer_len: 生成 token 数量\n",
    "\n",
    "    返回：\n",
    "    - record: Tensor，只包含新增的 token ids（不含 prompt 部分）\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # shape: [1, L_prompt]\n",
    "    generated_ids = input_ids.clone()\n",
    "    record = torch.empty((1, 0), dtype=torch.long, device=generated_ids.device)\n",
    "\n",
    "    for step in tqdm(range(answer_len)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated_ids, output_hidden_states=True, return_dict=True)\n",
    "            H_cur = outputs.hidden_states[-1]  # shape: [1, cur_len, hidden_size]\n",
    "\n",
    "        last_H = H_cur[:, -1, :] + delta.squeeze(1)  # 加扰动\n",
    "        logits = torch.matmul(last_H, model.lm_head.weight.T)  # shape: [1, vocab_size]\n",
    "        next_token_id = torch.argmax(logits, dim=-1)  # shape: [1]\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "        record = torch.cat([record, next_token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def compare_delta_generation(model, tokenizer, prompt, delta, answer_len=200):\n",
    "    \"\"\"\n",
    "    对比：带 delta 引导 与 无 delta 引导 的生成结果。\n",
    "\n",
    "    参数：\n",
    "    - model: 加载好的语言模型\n",
    "    - tokenizer: 分词器\n",
    "    - prompt: 输入 prompt（str）\n",
    "    - delta: 引导扰动向量（Tensor，shape=[1, 1, hidden_size]）\n",
    "    - answer_len: 生成 token 数量（默认 200）\n",
    "\n",
    "    返回：\n",
    "    - text_with_delta: 使用 delta 生成的文本\n",
    "    - text_no_delta: 不使用 delta 生成的文本\n",
    "    \"\"\"\n",
    "    print(\"✅ 使用 delta 生成文本...\")\n",
    "    record_with_delta = generate_by_H(model, prompt, tokenizer, delta, answer_len=answer_len)\n",
    "    text_with_delta = tokenizer.decode(record_with_delta[0], skip_special_tokens=True)\n",
    "    print(\"\\n📝 带 delta 引导的生成输出：\\n\")\n",
    "    print(text_with_delta)\n",
    "\n",
    "    # 构建全零 delta（对照组）\n",
    "    hidden_size = delta.shape[-1]\n",
    "    delta_zeros = torch.zeros_like(delta)\n",
    "\n",
    "    print(\"\\n✅ 使用 zero delta 生成文本...\")\n",
    "    record_no_delta = generate_by_H(model, prompt, tokenizer, delta_zeros, answer_len=answer_len)\n",
    "    text_no_delta = tokenizer.decode(record_no_delta[0], skip_special_tokens=True)\n",
    "    print(\"\\n📝 无 delta 引导的生成输出：\\n\")\n",
    "    print(text_no_delta)\n",
    "\n",
    "    return text_with_delta, text_no_delta\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.254962200Z",
     "start_time": "2025-06-05T16:01:07.238962900Z"
    }
   },
   "id": "bcbdf3f7f717d6b1",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 确认一下输入的Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463b6d1c2ea2d5bf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'请你详细介绍一下西湖。'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:07.288964500Z",
     "start_time": "2025-06-05T16:01:07.251961700Z"
    }
   },
   "id": "4d4871d4115c78a0",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加 delta 和不加 delta 的对比试验\n",
    "\n",
    "我们通过对比以下三种设置，观察生成内容的差异：\n",
    "\n",
    "- ❌ 无 delta：原始模型，直接基于 prompt 生成\n",
    "- ✅ delta（step=3）：训练 3 步得到的扰动，引导生成\n",
    "- ✅ delta（step=10）：训练 10 步得到的扰动，引导生成\n",
    "\n",
    "通过比较输出文本，可以分析 delta 是否有效提升了生成的一致性、主题聚焦程度或风格控制能力。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae3cbe46ca2f8f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 66.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      " 西湖，位于中国浙江省杭州市西湖区，是中国著名的风景名胜区之一，也是中国十大名胜之一。西湖以其秀丽的自然风光和丰富的历史文化而闻名于世。以下是关于西湖的详细介绍：\n",
      "\n",
      "### 1. **地理位置**\n",
      "西湖位于杭州市西湖区，地处钱塘江畔，是杭州的标志性景点之一。西湖的总面积约为13.7平方公里，是杭州的“心脏”所在。\n",
      "\n",
      "### 2. **自然风光**\n",
      "西湖的自然风光以湖光山色、花木繁茂、水鸟翔集而著称。西湖的湖面宽阔，湖岸线曲折，湖中有许多岛屿和小岛，如断桥、雷峰塔、断桥残雪等。湖中有许多著名的景点，如断桥残雪、苏堤春晓、平湖秋月、雷峰夕照等。\n",
      "\n",
      "### 3. **历史文化**\n",
      "西湖是中国古代四大名园之一，也是中国四大\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " 西湖，位于中国浙江省杭州市西湖区，是中国著名的风景名胜区之一，也是中国十大名胜之一。西湖以其秀丽的自然风光和丰富的历史文化而闻名于世，是中国古代四大名园之一。西湖的名称来源于西湖边的湖水，而西湖的名称则来源于西湖边的湖水。西湖的面积约为38.7平方公里，湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 你已经加载好的 delta\n",
    "# delta = torch.load(\"delta.pt\").to(model.device)\n",
    "\n",
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta,\n",
    "    answer_len=200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:13.224695300Z",
     "start_time": "2025-06-05T16:01:07.266962900Z"
    }
   },
   "id": "4389f168a7f722c2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 66.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      " 西湖，位于中国浙江省杭州市西湖区，是中国著名的风景名胜区之一，也是中国十大名胜之一。西湖以其秀丽的自然风光和丰富的历史文化而闻名于世。西湖的名称来源于西湖边的湖水，而西湖的名称则来源于西湖边的湖水。西湖的面积约为38.5平方公里，湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " 西湖，位于中国浙江省杭州市西湖区，是中国著名的风景名胜区之一，也是中国十大名胜之一。西湖以其秀丽的自然风光和丰富的历史文化而闻名于世，是中国古代四大名园之一。西湖的名称来源于西湖边的湖水，而西湖的名称则来源于西湖边的湖水。西湖的面积约为38.7平方公里，湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta_10,\n",
    "    answer_len=200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:19.241347400Z",
     "start_time": "2025-06-05T16:01:13.223694800Z"
    }
   },
   "id": "7e059f505835dad9",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 64.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      "西湖，位于中国浙江省杭州市西湖区，是中国西湖的所在地。西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖西湖，西湖\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 63.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " 西湖，位于中国浙江省杭州市西湖区，是中国著名的风景名胜区之一，也是中国十大名胜之一。西湖以其秀丽的自然风光和丰富的历史文化而闻名于世，是中国古代四大名园之一。西湖的名称来源于西湖边的湖水，而西湖的名称则来源于西湖边的湖水。西湖的面积约为38.7平方公里，湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖水面积约为10.5平方公里，湖水面积约为28平方公里。西湖的湖\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta_30,\n",
    "    answer_len=300\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:01:28.735436Z",
     "start_time": "2025-06-05T16:01:19.241347400Z"
    }
   },
   "id": "bf9e2bafd51ec4ab",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 对比试验2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcb3ed41580cac08"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"Please describe the company's current strategic direction and future development plans.\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:03:03.251932500Z",
     "start_time": "2025-06-05T16:03:03.242933500Z"
    }
   },
   "id": "213d51bce4afc030",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## 得到 H state\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"][0]  # 去掉 batch 维度\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# === 前向传播，输出包含 hidden_states ===\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "H = outputs.hidden_states[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:04:01.703857200Z",
     "start_time": "2025-06-05T16:04:01.662639500Z"
    }
   },
   "id": "84440ab18b970dca",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training delta: 100%|██████████| 3/3 [00:00<00:00, 14.03it/s]\n",
      "Training delta: 100%|██████████| 10/10 [00:00<00:00, 181.82it/s]\n",
      "Training delta: 100%|██████████| 30/30 [00:00<00:00, 191.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 执行训练\n",
    "delta = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=3)\n",
    "\n",
    "delta_10 = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=10)\n",
    "\n",
    "delta_30 = train_delta_from_H(model=model, tokenizer=tokenizer, prompt=prompt, H_state=H, step=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:04:04.079303100Z",
     "start_time": "2025-06-05T16:04:03.640430Z"
    }
   },
   "id": "52d3e22e75096e88",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 63.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on expanding its market share and increasing its revenue. The company has identified several key areas for growth, including expanding its product line, improving its customer service, and increasing its marketing efforts. The company has also identified several potential growth opportunities, including entering new markets and developing new products. The company's future development plans include continuing to invest in research and development to improve its products and services, expanding its distribution network to reach more customers, and increasing its marketing efforts to increase brand awareness and customer loyalty. The company also plans to continue to focus on its core competencies and maintain its competitive advantage in the industry.Human resources management is a critical function in any organization, and it plays a vital role in ensuring that the organization has the right people in the right positions. In this article, we will discuss the importance of human resources management and how it can help organizations achieve their goals.\n",
      "\n",
      "Human resources management is the process of managing an organization's human resources, including\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on expanding its market share and increasing its revenue. The company has identified several key areas for growth, including expanding its product line, improving its customer service, and increasing its marketing efforts. The company has also identified several potential growth opportunities, including entering new markets and developing new products. The company has also identified several areas for improvement, including improving its supply chain and reducing its costs. The company has also identified several potential risks, including economic downturns and competition from other companies. The company has also identified several potential opportunities, including new technologies and changing consumer preferences. The company has also identified several potential challenges, including regulatory changes and political instability. The company has also identified several potential partnerships, including with other companies and organizations. The company has also identified several potential investments, including in research and development and in new markets. The company has also identified several potential acquisitions, including in new product lines and in new markets. The company has also identified several potential divest\n",
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 65.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on expanding its market share and increasing its revenue. The company has identified several key areas for growth, including expanding its product line, improving its customer service, and increasing its marketing efforts. The company has also identified several potential growth opportunities, including entering new markets and developing new products. The company's future development plans include continuing to invest in research and development to improve its products and services, expanding its distribution network to reach more customers, and increasing its marketing efforts to increase brand awareness and customer loyalty. The company also plans to continue to focus on its core competencies and maintain its competitive advantage in the industry.Human resources management is a critical function in any organization, and it plays a vital role in ensuring that the organization has the right people in the right positions. In this article, we will discuss the importance of human resources management and how it can help organizations achieve their goals. Human resources management is the process of managing the organization's human resources, including\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on expanding its market share and increasing its revenue. The company has identified several key areas for growth, including expanding its product line, improving its customer service, and increasing its marketing efforts. The company has also identified several potential growth opportunities, including entering new markets and developing new products. The company has also identified several areas for improvement, including improving its supply chain and reducing its costs. The company has also identified several potential risks, including economic downturns and competition from other companies. The company has also identified several potential opportunities, including new technologies and changing consumer preferences. The company has also identified several potential challenges, including regulatory changes and political instability. The company has also identified several potential partnerships, including with other companies and organizations. The company has also identified several potential investments, including in research and development and in new markets. The company has also identified several potential acquisitions, including in new product lines and in new markets. The company has also identified several potential divest\n",
      "✅ 使用 delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 带 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on developing new products and services that meet the evolving needs of its customers. The company's future development plans include expanding its product line to include more advanced and innovative technologies, increasing its marketing efforts to reach a wider audience, and improving its customer service to enhance the overall customer experience. The company also plans to invest in research and development to stay ahead of the competition and develop new products and services that will meet the future needs of its customers.Human resources department is responsible for recruiting and developing new employees. The company's current recruitment strategy is to focus on attracting top talent from various industries and regions. The company's future development plans include expanding its recruitment efforts to include more diverse and talented candidates, improving its recruitment process to ensure that the company attracts the best candidates, and investing in training and development programs to help employees grow and develop their skills. The company also plans to invest in employee retention programs to ensure that employees remain with the company and contribute to the\n",
      "\n",
      "✅ 使用 zero delta 生成文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 无 delta 引导的生成输出：\n",
      "\n",
      " The company's current strategic direction is to focus on expanding its market share and increasing its revenue. The company has identified several key areas for growth, including expanding its product line, improving its customer service, and increasing its marketing efforts. The company has also identified several potential growth opportunities, including entering new markets and developing new products. The company has also identified several areas for improvement, including improving its supply chain and reducing its costs. The company has also identified several potential risks, including economic downturns and competition from other companies. The company has also identified several potential opportunities, including new technologies and changing consumer preferences. The company has also identified several potential challenges, including regulatory changes and political instability. The company has also identified several potential partnerships, including with other companies and organizations. The company has also identified several potential investments, including in research and development and in new markets. The company has also identified several potential acquisitions, including in new product lines and in new markets. The company has also identified several potential divest\n"
     ]
    }
   ],
   "source": [
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta,\n",
    "    answer_len=200\n",
    ")\n",
    "\n",
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta_10,\n",
    "    answer_len=200\n",
    ")\n",
    "\n",
    "text_with, text_without = compare_delta_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    delta=delta_30,\n",
    "    answer_len=200\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:04:39.468531300Z",
     "start_time": "2025-06-05T16:04:21.243065800Z"
    }
   },
   "id": "9d2b35f01eee658e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SLOT-Qwen3_final.ipynb to html\n",
      "[NbConvertApp] Writing 376372 bytes to SLOT-Qwen3_final.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html SLOT-Qwen3_final.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T16:28:23.097111100Z",
     "start_time": "2025-06-05T16:28:21.015511Z"
    }
   },
   "id": "e75d62da549423c0",
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
