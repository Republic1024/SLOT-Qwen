# delta_trainer.py
import torch
import torch.nn as nn
from tqdm import tqdm


def get_empty_delta(model, tokenizer, prompt, H_state):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    hidden_size = H_state.size(-1)
    delta = nn.Parameter(torch.zeros((1, 1, hidden_size), device=H_state.device, requires_grad=True))

    return delta


def train_delta_from_H(model, tokenizer, prompt, H_state, step=3, lr=1e-2):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]
    current_ids = input_ids[:, :-1]
    target_ids = input_ids[:, 1:]

    hidden_size = H_state.size(-1)
    delta = nn.Parameter(torch.zeros((1, 1, hidden_size), device=H_state.device, requires_grad=True))
    if step == 0:
        return delta

    optimizer = torch.optim.Adam([delta], lr=lr)
    loss_fn = nn.CrossEntropyLoss()
    loss_log = []

    for i in range(step):
        optimizer.zero_grad()
        delta_broadcast = delta.expand(H_state[:, :-1, :].shape)
        adjusted_H = H_state[:, :-1, :] + delta_broadcast
        logits = torch.matmul(adjusted_H, model.lm_head.weight.T)
        loss = loss_fn(logits.view(-1, logits.size(-1)), target_ids.view(-1))
        loss.backward()
        optimizer.step()
        loss_log.append(loss.item())
    return delta


def generate_by_H(model, prompt, tokenizer, delta, answer_len=100):
    """
    åŸºäºéšè—çŠ¶æ€ H æ·»åŠ æ‰°åŠ¨ delta çš„æ–¹å¼è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚

    å‚æ•°ï¼š
    - model: LLM æ¨¡å‹
    - prompt: è¾“å…¥æç¤ºè¯
    - tokenizer: åˆ†è¯å™¨
    - delta: æ‰°åŠ¨å¼ é‡ï¼Œshape=[1, 1, hidden_size]
    - answer_len: ç”Ÿæˆ token æ•°é‡

    è¿”å›ï¼š
    - record: Tensorï¼ŒåªåŒ…å«æ–°å¢çš„ token idsï¼ˆä¸å« prompt éƒ¨åˆ†ï¼‰
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]  # shape: [1, L_prompt]
    generated_ids = input_ids.clone()
    record = torch.empty((1, 0), dtype=torch.long, device=generated_ids.device)

    for step in tqdm(range(answer_len)):
        with torch.no_grad():
            outputs = model(input_ids=generated_ids, output_hidden_states=True, return_dict=True)
            H_cur = outputs.hidden_states[-1]  # shape: [1, cur_len, hidden_size]

        last_H = H_cur[:, -1, :] + delta.squeeze(1)  # åŠ æ‰°åŠ¨
        logits = torch.matmul(last_H, model.lm_head.weight.T)  # shape: [1, vocab_size]
        next_token_id = torch.argmax(logits, dim=-1)  # shape: [1]

        generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)
        record = torch.cat([record, next_token_id.unsqueeze(0)], dim=-1)

    record_txt = tokenizer.decode(record[0], skip_special_tokens=True)
    return record_txt


import torch
from tqdm import tqdm


def generate_by_H_fast(model, prompt, tokenizer, delta, answer_len=100):
    """
    ä½¿ç”¨ past_key_values åŠ é€Ÿç”Ÿæˆï¼ŒåŸºäºéšè—çŠ¶æ€ H æ·»åŠ æ‰°åŠ¨ deltaã€‚

    å‚æ•°ï¼š
    - model: æ”¯æŒ use_cache çš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰
    - prompt: è¾“å…¥æç¤ºæ–‡æœ¬
    - tokenizer: åˆ†è¯å™¨
    - delta: shape=[1, 1, hidden_size] çš„æ‰°åŠ¨å¼ é‡
    - answer_len: ç”Ÿæˆ token æ•°é‡

    è¿”å›ï¼š
    - record_txt: è§£ç åçš„ç”Ÿæˆæ–‡æœ¬ï¼ˆä¸å« promptï¼‰
    """
    # ç¼–ç  promptï¼Œè·å–é¦–ä¸ª token
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]  # [1, L]

    # ç¬¬ä¸€æ­¥ï¼šè·å– initial past_key_values
    with torch.no_grad():
        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)
        past_key_values = outputs.past_key_values

        # å–æœ€åä¸€ä¸ª token çš„ hidden state åŠ  delta
        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]
        logits = torch.matmul(H_last, model.lm_head.weight.T)  # [1, vocab_size]
        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]

    # åˆå§‹åŒ–ç»“æœ
    generated = [next_token_id]

    # åç»­ token é€æ­¥ç”Ÿæˆ
    for _ in tqdm(range(answer_len - 1), desc="Generating"):
        with torch.no_grad():
            outputs = model(
                input_ids=next_token_id,
                past_key_values=past_key_values,
                return_dict=True,
                output_hidden_states=True,
                use_cache=True
            )
            past_key_values = outputs.past_key_values
            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]
            logits = torch.matmul(H_last, model.lm_head.weight.T)
            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]

        generated.append(next_token_id)

    # æ‹¼æ¥æ‰€æœ‰ç”Ÿæˆ tokenï¼ˆä¸å« promptï¼‰
    gen_ids = torch.cat(generated, dim=-1)  # [1, T]
    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
    return record_txt


def evaluate_slot_ceval(model, tokenizer, delta, example, max_len=20, verbose=True):
    """
    ç”¨äºåŸºäº SLOT+Delta æ–¹æ³•è¯„ä¼°é€‰æ‹©é¢˜é—®é¢˜ï¼ˆç¬¦åˆ C-Eval æ ¼å¼ï¼‰ã€‚

    å‚æ•°ï¼š
    - model, tokenizer: åŠ è½½å¥½çš„æ¨¡å‹ä¸åˆ†è¯å™¨
    - delta: ç»è®­ç»ƒåçš„æ‰°åŠ¨å¼ é‡
    - example: å•ä¸ª C-Eval æ ¼å¼é¢˜ç›®å­—å…¸ï¼Œå¦‚ï¼š
        {
            'id': 0,
            'question': 'ä½¿ç”¨ä½å¡«å……æ–¹æ³•ï¼Œä»¥01111110ä¸ºä½é¦–flagï¼Œæ•°æ®ä¸º011011111111111111110010ï¼Œæ±‚é—®ä¼ é€æ—¶è¦æ·»åŠ å‡ ä¸ª0____',
            'A': '1',
            'B': '2',
            'C': '3',
            'D': '4',
            'answer': 'C',
            'explanation': ''
        }
    - max_len: ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 100ï¼‰
    - verbose: æ˜¯å¦æ‰“å°ä¸­é—´æ¨ç†æ–‡æœ¬

    è¿”å›ï¼š
    - predict_option: æ¨¡å‹é¢„æµ‹çš„é€‰é¡¹ï¼ˆå¦‚ 'A', 'B', 'C'ï¼‰
    - is_correct: æ˜¯å¦é¢„æµ‹æ­£ç¡®
    """
    # === æ„å»º Prompt ===
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·ä½ é˜…è¯»é¢˜ç›®å¹¶é€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚

é¢˜ç›®ï¼š{example['question']}

é€‰é¡¹ï¼š
A. {example['A']}
B. {example['B']}
C. {example['C']}
D. {example['D']}

ç­”æ¡ˆæ˜¯ï¼š"""

    # === ç”Ÿæˆç­”æ¡ˆæ–‡æœ¬ ===
    output_text = generate_by_H(model, prompt, tokenizer, delta, answer_len=max_len)

    if verbose:
        print("ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\n", output_text)

    # === åˆ¤æ–­å“ªä¸ªé€‰é¡¹è¢«æåŠ ===
    predict_option = None
    for option in ['A', 'B', 'C', 'D']:
        if option in output_text:
            predict_option = option
            break

    is_correct = (predict_option == example['answer'])
    return predict_option, is_correct


def generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=100):
    """
    åŸºäºéšè—çŠ¶æ€ H æ·»åŠ æ‰°åŠ¨ delta çš„æ–¹å¼è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œæ”¯æŒ <eos> æå‰ç»ˆæ­¢ã€‚

    å‚æ•°ï¼š
    - model: LLM æ¨¡å‹
    - prompt: è¾“å…¥æç¤ºè¯
    - tokenizer: åˆ†è¯å™¨
    - delta: æ‰°åŠ¨å¼ é‡ï¼Œshape=[1, 1, hidden_size]
    - answer_len: æœ€å¤šç”Ÿæˆ token æ•°é‡

    è¿”å›ï¼š
    - record_txt: è§£ç åçš„ç”Ÿæˆæ–‡æœ¬ï¼ˆä¸å« prompt éƒ¨åˆ†ï¼‰
    """
    eos_token_id = tokenizer.eos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]  # shape: [1, L_prompt]
    generated_ids = input_ids.clone()
    record = torch.empty((1, 0), dtype=torch.long, device=generated_ids.device)

    for _ in range(answer_len):
        with torch.no_grad():
            outputs = model(input_ids=generated_ids, output_hidden_states=True, return_dict=True)
            H_cur = outputs.hidden_states[-1]  # shape: [1, cur_len, hidden_size]

        last_H = H_cur[:, -1, :] + delta.squeeze(1)
        logits = torch.matmul(last_H, model.lm_head.weight.T)
        next_token_id = torch.argmax(logits, dim=-1)

        if next_token_id.item() == eos_token_id:
            break

        generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)
        record = torch.cat([record, next_token_id.unsqueeze(0)], dim=-1)

    record_txt = tokenizer.decode(record[0], skip_special_tokens=True)
    return record_txt


def evaluate_slot_ceval_eos(model, tokenizer, delta, example, max_len=20, verbose=True):
    """
    åŸºäº generate_by_H_eos çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº C-Eval å•é€‰é¢˜ç›®ã€‚

    è¿”å›ï¼š
    - predict_option: é¢„æµ‹é€‰é¡¹ï¼Œå¦‚ 'A'
    - is_correct: æ˜¯å¦é¢„æµ‹æ­£ç¡®
    """
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·ä½ é˜…è¯»é¢˜ç›®å¹¶é€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚

é¢˜ç›®ï¼š{example['question']}

é€‰é¡¹ï¼š
A. {example['A']}
B. {example['B']}
C. {example['C']}
D. {example['D']}

ç­”æ¡ˆæ˜¯ï¼š"""

    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)

    if verbose:
        print("ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\n", output_text)

    predict_option = None
    for option in ['A', 'B', 'C', 'D']:
        if option in output_text:
            predict_option = option
            break

    is_correct = (predict_option == example['answer'])
    return predict_option, is_correct


def evaluate_slot_ceval_eos_2(model, tokenizer, delta, example, max_len=20, verbose=True):
    """
    åŸºäº generate_by_H_eos çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº C-Eval å•é€‰é¢˜ç›®ã€‚

    è¿”å›ï¼š
    - predict_option: é¢„æµ‹é€‰é¡¹ï¼Œå¦‚ 'A'
    - is_correct: æ˜¯å¦é¢„æµ‹æ­£ç¡®
    """
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·ä½ é˜…è¯»é¢˜ç›®å¹¶é€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚

é¢˜ç›®ï¼š{example['question']}

é€‰é¡¹ï¼š
A. {example['A']}
B. {example['B']}
C. {example['C']}
D. {example['D']}

ç­”æ¡ˆæ˜¯ï¼š"""

    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)

    if verbose:
        print("ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\n", output_text)

    predict_option = None
    for option in ['A', 'B', 'C', 'D']:
        if option in output_text:
            predict_option = option
            break

    print(predict_option, example['answer'])
    is_correct = (predict_option == example['answer'])
    return predict_option, predict_option, example['answer']
