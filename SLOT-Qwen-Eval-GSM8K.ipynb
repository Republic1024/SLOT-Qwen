{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3b985ed9d334b8599bc0ae6c415ec1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"./Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "model_path = r\"./Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:25.014043800Z",
     "start_time": "2025-06-06T16:38:19.294845700Z"
    }
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "116c918538834e69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "from delta_trainer import train_delta_from_H\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset  # ç¡®ä¿è¿™ä¸€è¡Œåœ¨æ‚¨çš„ä»£ç ä¸­å­˜åœ¨\n",
    "\n",
    "\n",
    "# å‡è®¾ generate_by_H_eos_fast çš„å®šä¹‰å¦‚æ‚¨æ‰€æä¾›\n",
    "\n",
    "# å‡è®¾ train_delta_from_H çš„å®šä¹‰å¦‚ä¸‹ (è¯·æ ¹æ®æ‚¨çš„å®é™…å®ç°å¡«å……)\n",
    "# ä¸ºäº†æµ‹è¯•ç›®çš„ï¼Œæˆ‘å°†æä¾›ä¸€ä¸ªå ä½ç¬¦å®ç°\n",
    "\n",
    "\n",
    "def generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ past_key_values åŠ é€Ÿï¼Œæ”¯æŒ eos æˆªæ–­çš„ H å±‚æ‰°åŠ¨ç”Ÿæˆã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - model: æ”¯æŒ use_cache çš„ decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT ç³»åˆ—ï¼‰\n",
    "    - prompt: è¾“å…¥æ–‡æœ¬\n",
    "    - tokenizer: åˆ†è¯å™¨\n",
    "    - delta: shape=[1, 1, hidden_size] çš„æ‰°åŠ¨å¼ é‡\n",
    "    - answer_len: æœ€å¤šç”Ÿæˆ token æ•°\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - record_txt: è§£ç åçš„æ–‡æœ¬ï¼ˆä¸å« prompt éƒ¨åˆ†ï¼‰\n",
    "    \"\"\"\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # [1, L_prompt]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # åˆå§‹åŒ–æ¨ç†ï¼Œç¼“å­˜ key_values\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # é¦–ä¸ªæ‰°åŠ¨ + ç”Ÿæˆ\n",
    "        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]\n",
    "        logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    record = [next_token_id]  # æ”¶é›†ç”Ÿæˆ token\n",
    "\n",
    "    for _ in range(answer_len - 1):  # å·²ç”Ÿæˆ 1 ä¸ªï¼Œæœ€å¤šç”Ÿæˆ answer_len ä¸ª\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=next_token_id,\n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)\n",
    "            logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        record.append(next_token_id)\n",
    "\n",
    "    # æ‹¼æ¥ç”Ÿæˆåºåˆ—ï¼ˆä¸å« promptï¼‰\n",
    "    gen_ids = torch.cat(record, dim=-1)  # [1, T]\n",
    "    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return record_txt\n",
    "\n",
    "\n",
    "def evaluate_gsm8k_eos(model, tokenizer, delta, example, max_len=200, verbose=True):\n",
    "    \"\"\"\n",
    "    åŸºäº generate_by_H_eos_fast çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº GSM8K æ•°æ®é›†ã€‚\n",
    "    ç›®æ ‡æ˜¯ç”Ÿæˆè§£ç­”å’Œæœ€ç»ˆçš„æ•°å­—ç­”æ¡ˆã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - generated_text: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬ï¼ˆè§£ç­” + ç­”æ¡ˆï¼‰\n",
    "    - extracted_answer: ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–çš„æ•°å­—ç­”æ¡ˆ\n",
    "    - actual_answer: å®é™…çš„æ•°å­—ç­”æ¡ˆ\n",
    "    - is_correct: ç­”æ¡ˆæ˜¯å¦æ­£ç¡®\n",
    "    \"\"\"\n",
    "    prompt = f\"é—®é¢˜: {example['question']}\\nç­”æ¡ˆæ˜¯:\"  # GSM8K prompt ç¤ºä¾‹\n",
    "\n",
    "    generated_text = generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\\n\", generated_text)\n",
    "\n",
    "    # ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–æ•°å­—ç­”æ¡ˆ\n",
    "    # GSM8K çš„ç­”æ¡ˆé€šå¸¸åœ¨ \"\\\\n#### \" ä¹‹å\n",
    "    extracted_answer = None\n",
    "    if \"####\" in generated_text:\n",
    "        try:\n",
    "            # æ‰¾åˆ°æœ€åä¸€ä¸ª #### åçš„å†…å®¹\n",
    "            answer_part = generated_text.split(\"####\")[-1].strip()\n",
    "            # å°è¯•å°†ç­”æ¡ˆéƒ¨åˆ†è½¬æ¢ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°\n",
    "            extracted_answer = float(answer_part)\n",
    "        except ValueError:\n",
    "            pass  # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œåˆ™ä¿æŒ None\n",
    "\n",
    "    # GSM8K çš„çœŸå®ç­”æ¡ˆæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯èƒ½éœ€è¦è½¬æ¢ä¸ºæ•°å­—è¿›è¡Œæ¯”è¾ƒ\n",
    "    actual_answer = None\n",
    "    try:\n",
    "        actual_answer = float(example['answer'].split(\"####\")[-1].strip())\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    is_correct = (extracted_answer is not None and actual_answer is not None and abs(\n",
    "        extracted_answer - actual_answer) < 1e-6)  # ä½¿ç”¨ä¸€ä¸ªå°çš„å®¹å·®è¿›è¡Œæµ®ç‚¹æ•°æ¯”è¾ƒ\n",
    "\n",
    "    return generated_text, extracted_answer, actual_answer, is_correct\n",
    "\n",
    "\n",
    "# MODIFIED: Accepts 'dataset_to_evaluate' as a parameter\n",
    "def eval_gsm8k_dataset(model, tokenizer, dataset_to_evaluate, step=3, max_len=200, lr=1e-2):\n",
    "    # 'dataset_to_evaluate' is now passed in, no need to load it here\n",
    "    gsm8k_data = dataset_to_evaluate # Use the passed dataset\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_sheet = []\n",
    "\n",
    "    for ex in tqdm(gsm8k_data): # Iterate over the passed dataset\n",
    "        prompt = f\"é—®é¢˜: {ex['question']}\\nç­”æ¡ˆæ˜¯:\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step, lr=lr)\n",
    "\n",
    "        gen_txt, pred_ans, actual_ans, is_correct = evaluate_gsm8k_eos(model=model, tokenizer=tokenizer,\n",
    "                                                                       delta=delta, example=ex,\n",
    "                                                                       max_len=max_len, verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        results_sheet.append([prompt, gen_txt, pred_ans, actual_ans, is_correct, \"gsm8k\"])\n",
    "\n",
    "    print(f\"ğŸ¯ GSM8K Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return results_sheet\n",
    "\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:26.212063900Z",
     "start_time": "2025-06-06T16:38:26.195065400Z"
    }
   },
   "id": "2125aaebf311ec2c",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming 'model' and 'tokenizer' are already loaded\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_name = \"your_model_name_here\" # e.g., \"gpt2\", \"THUDM/chatglm3-6b\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½ GSM8K æµ‹è¯•æ•°æ®é›†\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "gsm8k_test_data = dataset[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:32:50.336170100Z",
     "start_time": "2025-06-06T16:32:48.714464200Z"
    }
   },
   "id": "997861e12676ff9e",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'dataset': <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Type of 'gsm8k_test_data': <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of 'dataset': {type(dataset)}\")\n",
    "print(f\"Type of 'gsm8k_test_data': {type(gsm8k_test_data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:34:09.427522300Z",
     "start_time": "2025-06-06T16:34:09.418522400Z"
    }
   },
   "id": "a061f4562765c00c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'gsm8k_test_data_subset': <class 'datasets.arrow_dataset.Dataset'>\n",
      "Number of examples in subset: 3\n",
      "Type of first element in subset: <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c775b9136c894a24927c291ce82953ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ GSM8K Accuracy (per-question delta): 0/3 = 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Use .select(range(100)) to get a Dataset object with the first 100 examples\n",
    "gsm8k_test_data_subset = gsm8k_test_data.select(range(3))\n",
    "\n",
    "# Verify the type of the subset (optional, but good for debugging)\n",
    "print(f\"Type of 'gsm8k_test_data_subset': {type(gsm8k_test_data_subset)}\")\n",
    "print(f\"Number of examples in subset: {len(gsm8k_test_data_subset)}\")\n",
    "print(f\"Type of first element in subset: {type(gsm8k_test_data_subset[0])}\")\n",
    "\n",
    "\n",
    "# Now call your evaluation function with the subset\n",
    "gsm8k_results = eval_gsm8k_dataset(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_to_evaluate=gsm8k_test_data_subset, # Pass the Dataset subset directly\n",
    "    step=3,\n",
    "    max_len=200,\n",
    "    lr=1e-2\n",
    ")\n",
    "\n",
    "# You can now inspect gsm8k_results\n",
    "# print(gsm8k_results[:5]) # Print first 5 results for inspection"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:50.123778800Z",
     "start_time": "2025-06-06T16:38:37.955202900Z"
    }
   },
   "id": "c2f1ff0d970408b7",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   0  \\\n0  é—®é¢˜: Janetâ€™s ducks lay 16 eggs per day. She eat...   \n1  é—®é¢˜: A robe takes 2 bolts of blue fiber and hal...   \n2  é—®é¢˜: Josh decides to try flipping a house.  He ...   \n\n                                                   1     2        3      4  \\\n0   160\\næ­¥éª¤è§£é‡Š:\\n1. è®¡ç®—æ¯å¤©çš„é¸¡è›‹äº§é‡: 16 eggs/day\\n2. è®¡ç®—æ¯...  None     18.0  False   \n1   2 bolts of blue fiber and 1 bolt of white fib...  None      3.0  False   \n2   150,000\\né—®é¢˜: Josh decides to try flipping a h...  None  70000.0  False   \n\n       5  \n0  gsm8k  \n1  gsm8k  \n2  gsm8k  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>é—®é¢˜: Janetâ€™s ducks lay 16 eggs per day. She eat...</td>\n      <td>160\\næ­¥éª¤è§£é‡Š:\\n1. è®¡ç®—æ¯å¤©çš„é¸¡è›‹äº§é‡: 16 eggs/day\\n2. è®¡ç®—æ¯...</td>\n      <td>None</td>\n      <td>18.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>é—®é¢˜: A robe takes 2 bolts of blue fiber and hal...</td>\n      <td>2 bolts of blue fiber and 1 bolt of white fib...</td>\n      <td>None</td>\n      <td>3.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>é—®é¢˜: Josh decides to try flipping a house.  He ...</td>\n      <td>150,000\\né—®é¢˜: Josh decides to try flipping a h...</td>\n      <td>None</td>\n      <td>70000.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gsm8k_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:50.135947100Z",
     "start_time": "2025-06-06T16:38:50.121779300Z"
    }
   },
   "id": "8e6c1d803e35559c",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5355d20c8df3b341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
