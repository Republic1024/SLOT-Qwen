{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"./Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "# model_path = r\"./Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T15:49:39.926928100Z",
     "start_time": "2025-06-06T15:49:39.049217200Z"
    }
   },
   "id": "116c918538834e69",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "from delta_trainer import train_delta_from_H\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset # 确保这一行在您的代码中存在\n",
    "\n",
    "# 假设 generate_by_H_eos_fast 的定义如您所提供\n",
    "\n",
    "# 假设 train_delta_from_H 的定义如下 (请根据您的实际实现填充)\n",
    "# 为了测试目的，我将提供一个占位符实现\n",
    "\n",
    "\n",
    "def generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    使用 past_key_values 加速，支持 eos 截断的 H 层扰动生成。\n",
    "\n",
    "    参数：\n",
    "    - model: 支持 use_cache 的 decoder-only 模型（如 GPT 系列）\n",
    "    - prompt: 输入文本\n",
    "    - tokenizer: 分词器\n",
    "    - delta: shape=[1, 1, hidden_size] 的扰动张量\n",
    "    - answer_len: 最多生成 token 数\n",
    "\n",
    "    返回：\n",
    "    - record_txt: 解码后的文本（不含 prompt 部分）\n",
    "    \"\"\"\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # [1, L_prompt]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 初始化推理，缓存 key_values\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # 首个扰动 + 生成\n",
    "        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]\n",
    "        logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    record = [next_token_id]  # 收集生成 token\n",
    "\n",
    "    for _ in range(answer_len - 1):  # 已生成 1 个，最多生成 answer_len 个\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=next_token_id,\n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)\n",
    "            logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        record.append(next_token_id)\n",
    "\n",
    "    # 拼接生成序列（不含 prompt）\n",
    "    gen_ids = torch.cat(record, dim=-1)  # [1, T]\n",
    "    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return record_txt\n",
    "def evaluate_gsm8k_eos(model, tokenizer, delta, example, max_len=200, verbose=True):\n",
    "    \"\"\"\n",
    "    基于 generate_by_H_eos_fast 的评估函数，用于 GSM8K 数据集。\n",
    "    目标是生成解答和最终的数字答案。\n",
    "\n",
    "    返回：\n",
    "    - generated_text: 模型生成的完整文本（解答 + 答案）\n",
    "    - extracted_answer: 从生成文本中提取的数字答案\n",
    "    - actual_answer: 实际的数字答案\n",
    "    - is_correct: 答案是否正确\n",
    "    \"\"\"\n",
    "    prompt = f\"问题: {example['question']}\\n答案是:\" # GSM8K prompt 示例\n",
    "\n",
    "    generated_text = generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"🔍 模型生成结果:\\n\", generated_text)\n",
    "\n",
    "    # 从生成文本中提取数字答案\n",
    "    # GSM8K 的答案通常在 \"\\\\n#### \" 之后\n",
    "    extracted_answer = None\n",
    "    if \"####\" in generated_text:\n",
    "        try:\n",
    "            # 找到最后一个 #### 后的内容\n",
    "            answer_part = generated_text.split(\"####\")[-1].strip()\n",
    "            # 尝试将答案部分转换为整数或浮点数\n",
    "            extracted_answer = float(answer_part)\n",
    "        except ValueError:\n",
    "            pass # 如果无法转换为数字，则保持 None\n",
    "\n",
    "    # GSM8K 的真实答案是一个字符串，可能需要转换为数字进行比较\n",
    "    actual_answer = None\n",
    "    try:\n",
    "        actual_answer = float(example['answer'].split(\"####\")[-1].strip())\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    is_correct = (extracted_answer is not None and actual_answer is not None and abs(extracted_answer - actual_answer) < 1e-6) # 使用一个小的容差进行浮点数比较\n",
    "\n",
    "    return generated_text, extracted_answer, actual_answer, is_correct\n",
    "\n",
    "\n",
    "def eval_gsm8k_dataset(model, tokenizer, step=3, max_len=200, lr=1e-2): # 增加 model 和 tokenizer 参数\n",
    "    # 加载 GSM8K 测试数据集\n",
    "    dataset = load_dataset(\"parquet\", data_files={\n",
    "        \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "        \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "    })\n",
    "    gsm8k_test_data = dataset[\"test\"]\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_sheet = []\n",
    "\n",
    "    for ex in tqdm(gsm8k_test_data):\n",
    "        # === 构造每道题的 Prompt ===\n",
    "        # GSM8K 的 prompt 通常是问题本身，然后模型生成思考过程和答案\n",
    "        prompt = f\"问题: {ex['question']}\\n答案是:\"\n",
    "\n",
    "        # === 获取 H_state ===\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        # === 训练 delta（例如3步）===\n",
    "        # 这里的 train_delta_from_H 需要根据 GSM8K 的目标（生成正确的数字答案）来调整其内部损失函数\n",
    "        # 例如，可以尝试优化 delta 使模型生成下一个 token 的概率分布更接近正确答案的 token 序列\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step, lr=lr)\n",
    "\n",
    "        # === 推理与评估 ===\n",
    "        gen_txt, pred_ans, actual_ans, is_correct = evaluate_gsm8k_eos(model=model, tokenizer=tokenizer,\n",
    "                                                                       delta=delta, example=ex,\n",
    "                                                                       max_len=max_len, verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        results_sheet.append([prompt, gen_txt, pred_ans, actual_ans, is_correct, \"gsm8k\"])\n",
    "\n",
    "    print(f\"🎯 GSM8K Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return results_sheet\n",
    "\n",
    "\n",
    "\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T15:49:41.056182700Z",
     "start_time": "2025-06-06T15:49:41.027184Z"
    }
   },
   "id": "2125aaebf311ec2c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gsm8k_results = eval_gsm8k_dataset(model, tokenizer, step=3, max_len=200, lr=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3449f7e5f300a661"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
