{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"./Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "# model_path = r\"./Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T15:49:39.926928100Z",
     "start_time": "2025-06-06T15:49:39.049217200Z"
    }
   },
   "id": "116c918538834e69",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "from delta_trainer import train_delta_from_H\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset # ç¡®ä¿è¿™ä¸€è¡Œåœ¨æ‚¨çš„ä»£ç ä¸­å­˜åœ¨\n",
    "\n",
    "# å‡è®¾ generate_by_H_eos_fast çš„å®šä¹‰å¦‚æ‚¨æ‰€æä¾›\n",
    "\n",
    "# å‡è®¾ train_delta_from_H çš„å®šä¹‰å¦‚ä¸‹ (è¯·æ ¹æ®æ‚¨çš„å®é™…å®ç°å¡«å……)\n",
    "# ä¸ºäº†æµ‹è¯•ç›®çš„ï¼Œæˆ‘å°†æä¾›ä¸€ä¸ªå ä½ç¬¦å®ç°\n",
    "\n",
    "\n",
    "def generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ past_key_values åŠ é€Ÿï¼Œæ”¯æŒ eos æˆªæ–­çš„ H å±‚æ‰°åŠ¨ç”Ÿæˆã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - model: æ”¯æŒ use_cache çš„ decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT ç³»åˆ—ï¼‰\n",
    "    - prompt: è¾“å…¥æ–‡æœ¬\n",
    "    - tokenizer: åˆ†è¯å™¨\n",
    "    - delta: shape=[1, 1, hidden_size] çš„æ‰°åŠ¨å¼ é‡\n",
    "    - answer_len: æœ€å¤šç”Ÿæˆ token æ•°\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - record_txt: è§£ç åçš„æ–‡æœ¬ï¼ˆä¸å« prompt éƒ¨åˆ†ï¼‰\n",
    "    \"\"\"\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # [1, L_prompt]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # åˆå§‹åŒ–æ¨ç†ï¼Œç¼“å­˜ key_values\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # é¦–ä¸ªæ‰°åŠ¨ + ç”Ÿæˆ\n",
    "        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]\n",
    "        logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    record = [next_token_id]  # æ”¶é›†ç”Ÿæˆ token\n",
    "\n",
    "    for _ in range(answer_len - 1):  # å·²ç”Ÿæˆ 1 ä¸ªï¼Œæœ€å¤šç”Ÿæˆ answer_len ä¸ª\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=next_token_id,\n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)\n",
    "            logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        record.append(next_token_id)\n",
    "\n",
    "    # æ‹¼æ¥ç”Ÿæˆåºåˆ—ï¼ˆä¸å« promptï¼‰\n",
    "    gen_ids = torch.cat(record, dim=-1)  # [1, T]\n",
    "    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return record_txt\n",
    "def evaluate_gsm8k_eos(model, tokenizer, delta, example, max_len=200, verbose=True):\n",
    "    \"\"\"\n",
    "    åŸºäº generate_by_H_eos_fast çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº GSM8K æ•°æ®é›†ã€‚\n",
    "    ç›®æ ‡æ˜¯ç”Ÿæˆè§£ç­”å’Œæœ€ç»ˆçš„æ•°å­—ç­”æ¡ˆã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - generated_text: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬ï¼ˆè§£ç­” + ç­”æ¡ˆï¼‰\n",
    "    - extracted_answer: ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–çš„æ•°å­—ç­”æ¡ˆ\n",
    "    - actual_answer: å®é™…çš„æ•°å­—ç­”æ¡ˆ\n",
    "    - is_correct: ç­”æ¡ˆæ˜¯å¦æ­£ç¡®\n",
    "    \"\"\"\n",
    "    prompt = f\"é—®é¢˜: {example['question']}\\nç­”æ¡ˆæ˜¯:\" # GSM8K prompt ç¤ºä¾‹\n",
    "\n",
    "    generated_text = generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\\n\", generated_text)\n",
    "\n",
    "    # ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–æ•°å­—ç­”æ¡ˆ\n",
    "    # GSM8K çš„ç­”æ¡ˆé€šå¸¸åœ¨ \"\\\\n#### \" ä¹‹å\n",
    "    extracted_answer = None\n",
    "    if \"####\" in generated_text:\n",
    "        try:\n",
    "            # æ‰¾åˆ°æœ€åä¸€ä¸ª #### åçš„å†…å®¹\n",
    "            answer_part = generated_text.split(\"####\")[-1].strip()\n",
    "            # å°è¯•å°†ç­”æ¡ˆéƒ¨åˆ†è½¬æ¢ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°\n",
    "            extracted_answer = float(answer_part)\n",
    "        except ValueError:\n",
    "            pass # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œåˆ™ä¿æŒ None\n",
    "\n",
    "    # GSM8K çš„çœŸå®ç­”æ¡ˆæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯èƒ½éœ€è¦è½¬æ¢ä¸ºæ•°å­—è¿›è¡Œæ¯”è¾ƒ\n",
    "    actual_answer = None\n",
    "    try:\n",
    "        actual_answer = float(example['answer'].split(\"####\")[-1].strip())\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    is_correct = (extracted_answer is not None and actual_answer is not None and abs(extracted_answer - actual_answer) < 1e-6) # ä½¿ç”¨ä¸€ä¸ªå°çš„å®¹å·®è¿›è¡Œæµ®ç‚¹æ•°æ¯”è¾ƒ\n",
    "\n",
    "    return generated_text, extracted_answer, actual_answer, is_correct\n",
    "\n",
    "\n",
    "def eval_gsm8k_dataset(model, tokenizer, step=3, max_len=200, lr=1e-2): # å¢åŠ  model å’Œ tokenizer å‚æ•°\n",
    "    # åŠ è½½ GSM8K æµ‹è¯•æ•°æ®é›†\n",
    "    dataset = load_dataset(\"parquet\", data_files={\n",
    "        \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "        \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "    })\n",
    "    gsm8k_test_data = dataset[\"test\"]\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_sheet = []\n",
    "\n",
    "    for ex in tqdm(gsm8k_test_data):\n",
    "        # === æ„é€ æ¯é“é¢˜çš„ Prompt ===\n",
    "        # GSM8K çš„ prompt é€šå¸¸æ˜¯é—®é¢˜æœ¬èº«ï¼Œç„¶åæ¨¡å‹ç”Ÿæˆæ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ\n",
    "        prompt = f\"é—®é¢˜: {ex['question']}\\nç­”æ¡ˆæ˜¯:\"\n",
    "\n",
    "        # === è·å– H_state ===\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        # === è®­ç»ƒ deltaï¼ˆä¾‹å¦‚3æ­¥ï¼‰===\n",
    "        # è¿™é‡Œçš„ train_delta_from_H éœ€è¦æ ¹æ® GSM8K çš„ç›®æ ‡ï¼ˆç”Ÿæˆæ­£ç¡®çš„æ•°å­—ç­”æ¡ˆï¼‰æ¥è°ƒæ•´å…¶å†…éƒ¨æŸå¤±å‡½æ•°\n",
    "        # ä¾‹å¦‚ï¼Œå¯ä»¥å°è¯•ä¼˜åŒ– delta ä½¿æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒæ›´æ¥è¿‘æ­£ç¡®ç­”æ¡ˆçš„ token åºåˆ—\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step, lr=lr)\n",
    "\n",
    "        # === æ¨ç†ä¸è¯„ä¼° ===\n",
    "        gen_txt, pred_ans, actual_ans, is_correct = evaluate_gsm8k_eos(model=model, tokenizer=tokenizer,\n",
    "                                                                       delta=delta, example=ex,\n",
    "                                                                       max_len=max_len, verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        results_sheet.append([prompt, gen_txt, pred_ans, actual_ans, is_correct, \"gsm8k\"])\n",
    "\n",
    "    print(f\"ğŸ¯ GSM8K Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return results_sheet\n",
    "\n",
    "\n",
    "\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T15:49:41.056182700Z",
     "start_time": "2025-06-06T15:49:41.027184Z"
    }
   },
   "id": "2125aaebf311ec2c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gsm8k_results = eval_gsm8k_dataset(model, tokenizer, step=3, max_len=200, lr=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3449f7e5f300a661"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
