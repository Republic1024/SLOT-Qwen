{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3b985ed9d334b8599bc0ae6c415ec1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"./Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "model_path = r\"./Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:25.014043800Z",
     "start_time": "2025-06-06T16:38:19.294845700Z"
    }
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "116c918538834e69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "from delta_trainer import train_delta_from_H\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset  # 确保这一行在您的代码中存在\n",
    "\n",
    "\n",
    "# 假设 generate_by_H_eos_fast 的定义如您所提供\n",
    "\n",
    "# 假设 train_delta_from_H 的定义如下 (请根据您的实际实现填充)\n",
    "# 为了测试目的，我将提供一个占位符实现\n",
    "\n",
    "\n",
    "def generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    使用 past_key_values 加速，支持 eos 截断的 H 层扰动生成。\n",
    "\n",
    "    参数：\n",
    "    - model: 支持 use_cache 的 decoder-only 模型（如 GPT 系列）\n",
    "    - prompt: 输入文本\n",
    "    - tokenizer: 分词器\n",
    "    - delta: shape=[1, 1, hidden_size] 的扰动张量\n",
    "    - answer_len: 最多生成 token 数\n",
    "\n",
    "    返回：\n",
    "    - record_txt: 解码后的文本（不含 prompt 部分）\n",
    "    \"\"\"\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # [1, L_prompt]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 初始化推理，缓存 key_values\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # 首个扰动 + 生成\n",
    "        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]\n",
    "        logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    record = [next_token_id]  # 收集生成 token\n",
    "\n",
    "    for _ in range(answer_len - 1):  # 已生成 1 个，最多生成 answer_len 个\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=next_token_id,\n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)\n",
    "            logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        record.append(next_token_id)\n",
    "\n",
    "    # 拼接生成序列（不含 prompt）\n",
    "    gen_ids = torch.cat(record, dim=-1)  # [1, T]\n",
    "    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return record_txt\n",
    "\n",
    "\n",
    "def evaluate_gsm8k_eos(model, tokenizer, delta, example, max_len=200, verbose=True):\n",
    "    \"\"\"\n",
    "    基于 generate_by_H_eos_fast 的评估函数，用于 GSM8K 数据集。\n",
    "    目标是生成解答和最终的数字答案。\n",
    "\n",
    "    返回：\n",
    "    - generated_text: 模型生成的完整文本（解答 + 答案）\n",
    "    - extracted_answer: 从生成文本中提取的数字答案\n",
    "    - actual_answer: 实际的数字答案\n",
    "    - is_correct: 答案是否正确\n",
    "    \"\"\"\n",
    "    prompt = f\"问题: {example['question']}\\n答案是:\"  # GSM8K prompt 示例\n",
    "\n",
    "    generated_text = generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"🔍 模型生成结果:\\n\", generated_text)\n",
    "\n",
    "    # 从生成文本中提取数字答案\n",
    "    # GSM8K 的答案通常在 \"\\\\n#### \" 之后\n",
    "    extracted_answer = None\n",
    "    if \"####\" in generated_text:\n",
    "        try:\n",
    "            # 找到最后一个 #### 后的内容\n",
    "            answer_part = generated_text.split(\"####\")[-1].strip()\n",
    "            # 尝试将答案部分转换为整数或浮点数\n",
    "            extracted_answer = float(answer_part)\n",
    "        except ValueError:\n",
    "            pass  # 如果无法转换为数字，则保持 None\n",
    "\n",
    "    # GSM8K 的真实答案是一个字符串，可能需要转换为数字进行比较\n",
    "    actual_answer = None\n",
    "    try:\n",
    "        actual_answer = float(example['answer'].split(\"####\")[-1].strip())\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    is_correct = (extracted_answer is not None and actual_answer is not None and abs(\n",
    "        extracted_answer - actual_answer) < 1e-6)  # 使用一个小的容差进行浮点数比较\n",
    "\n",
    "    return generated_text, extracted_answer, actual_answer, is_correct\n",
    "\n",
    "\n",
    "# MODIFIED: Accepts 'dataset_to_evaluate' as a parameter\n",
    "def eval_gsm8k_dataset(model, tokenizer, dataset_to_evaluate, step=3, max_len=200, lr=1e-2):\n",
    "    # 'dataset_to_evaluate' is now passed in, no need to load it here\n",
    "    gsm8k_data = dataset_to_evaluate # Use the passed dataset\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_sheet = []\n",
    "\n",
    "    for ex in tqdm(gsm8k_data): # Iterate over the passed dataset\n",
    "        prompt = f\"问题: {ex['question']}\\n答案是:\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step, lr=lr)\n",
    "\n",
    "        gen_txt, pred_ans, actual_ans, is_correct = evaluate_gsm8k_eos(model=model, tokenizer=tokenizer,\n",
    "                                                                       delta=delta, example=ex,\n",
    "                                                                       max_len=max_len, verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        results_sheet.append([prompt, gen_txt, pred_ans, actual_ans, is_correct, \"gsm8k\"])\n",
    "\n",
    "    print(f\"🎯 GSM8K Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return results_sheet\n",
    "\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:26.212063900Z",
     "start_time": "2025-06-06T16:38:26.195065400Z"
    }
   },
   "id": "2125aaebf311ec2c",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming 'model' and 'tokenizer' are already loaded\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_name = \"your_model_name_here\" # e.g., \"gpt2\", \"THUDM/chatglm3-6b\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载 GSM8K 测试数据集\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"./gsm8k/main/train-00000-of-00001.parquet\",\n",
    "    \"test\": \"./gsm8k/main/test-00000-of-00001.parquet\"\n",
    "})\n",
    "gsm8k_test_data = dataset[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:32:50.336170100Z",
     "start_time": "2025-06-06T16:32:48.714464200Z"
    }
   },
   "id": "997861e12676ff9e",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'dataset': <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Type of 'gsm8k_test_data': <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of 'dataset': {type(dataset)}\")\n",
    "print(f\"Type of 'gsm8k_test_data': {type(gsm8k_test_data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:34:09.427522300Z",
     "start_time": "2025-06-06T16:34:09.418522400Z"
    }
   },
   "id": "a061f4562765c00c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'gsm8k_test_data_subset': <class 'datasets.arrow_dataset.Dataset'>\n",
      "Number of examples in subset: 3\n",
      "Type of first element in subset: <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c775b9136c894a24927c291ce82953ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 GSM8K Accuracy (per-question delta): 0/3 = 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Use .select(range(100)) to get a Dataset object with the first 100 examples\n",
    "gsm8k_test_data_subset = gsm8k_test_data.select(range(3))\n",
    "\n",
    "# Verify the type of the subset (optional, but good for debugging)\n",
    "print(f\"Type of 'gsm8k_test_data_subset': {type(gsm8k_test_data_subset)}\")\n",
    "print(f\"Number of examples in subset: {len(gsm8k_test_data_subset)}\")\n",
    "print(f\"Type of first element in subset: {type(gsm8k_test_data_subset[0])}\")\n",
    "\n",
    "\n",
    "# Now call your evaluation function with the subset\n",
    "gsm8k_results = eval_gsm8k_dataset(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_to_evaluate=gsm8k_test_data_subset, # Pass the Dataset subset directly\n",
    "    step=3,\n",
    "    max_len=200,\n",
    "    lr=1e-2\n",
    ")\n",
    "\n",
    "# You can now inspect gsm8k_results\n",
    "# print(gsm8k_results[:5]) # Print first 5 results for inspection"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:50.123778800Z",
     "start_time": "2025-06-06T16:38:37.955202900Z"
    }
   },
   "id": "c2f1ff0d970408b7",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   0  \\\n0  问题: Janet’s ducks lay 16 eggs per day. She eat...   \n1  问题: A robe takes 2 bolts of blue fiber and hal...   \n2  问题: Josh decides to try flipping a house.  He ...   \n\n                                                   1     2        3      4  \\\n0   160\\n步骤解释:\\n1. 计算每天的鸡蛋产量: 16 eggs/day\\n2. 计算每...  None     18.0  False   \n1   2 bolts of blue fiber and 1 bolt of white fib...  None      3.0  False   \n2   150,000\\n问题: Josh decides to try flipping a h...  None  70000.0  False   \n\n       5  \n0  gsm8k  \n1  gsm8k  \n2  gsm8k  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>问题: Janet’s ducks lay 16 eggs per day. She eat...</td>\n      <td>160\\n步骤解释:\\n1. 计算每天的鸡蛋产量: 16 eggs/day\\n2. 计算每...</td>\n      <td>None</td>\n      <td>18.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>问题: A robe takes 2 bolts of blue fiber and hal...</td>\n      <td>2 bolts of blue fiber and 1 bolt of white fib...</td>\n      <td>None</td>\n      <td>3.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>问题: Josh decides to try flipping a house.  He ...</td>\n      <td>150,000\\n问题: Josh decides to try flipping a h...</td>\n      <td>None</td>\n      <td>70000.0</td>\n      <td>False</td>\n      <td>gsm8k</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gsm8k_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:38:50.135947100Z",
     "start_time": "2025-06-06T16:38:50.121779300Z"
    }
   },
   "id": "8e6c1d803e35559c",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5355d20c8df3b341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
