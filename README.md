

# SLOT-Qwen ðŸš€  
**Sample-specific Generation via Hidden-State Perturbation (Based on Qwen3-0.6B)**

This repository implements a simplified and practical version of the **SLOT** method â€” originally proposed in  
ðŸ“„ **"SLOT: Sample-specific Language Model Optimization at Test-time"**  
by *Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi*,  
presented by Westlake University, University of Washington, and USTC.

> This implementation adapts the SLOT idea to the [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B) model, demonstrating how small test-time optimization on hidden states can guide controllable generation without updating model parameters.

---

## ðŸ” What is SLOT?

SLOT (Sample-specific Language Model Optimization at Test-time) proposes a lightweight test-time adaptation strategy for language models:

- A small perturbation vector `delta` is trained on the prompt itself;
- This delta is added to the hidden states (e.g., last transformer layer) during generation;
- No fine-tuning of model weights is required;
- Works for arbitrary prompts, supports fast and local adaptation.

---

## âœ¨ Key Features

- **Parameter-free generation**: Frozen LLM, no weight updates.
- **Prompt-specific control**: Each prompt has its own delta.
- **Efficient**: Delta can be trained in just 3â€“10 steps.
- **Plug-and-play**: No model modification required.
- **Supports Qwen3-0.6B (Huggingface / ModelScope)**

---

## ðŸ› ï¸ Quick Start

### 1. Clone the repo and install dependencies
```bash
git clone https://github.com/repubic1024/SLOT-Qwen.git
cd SLOT-Qwen
pip install -r requirements.txt
```

### 2. Load Qwen3-0.6B

#### ðŸ”¹ From Huggingface

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B").cuda()
```

#### ðŸ”¹ From ModelScope

```python
from modelscope import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen3-0.6B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen3-0.6B", trust_remote_code=True).cuda()
```

---

## ðŸš€ How It Works

1. Encode prompt and get hidden states from last transformer layer.
2. Train a small delta vector `delta âˆˆ â„Â¹Ë£Â¹Ë£á´´` on next-token prediction.
3. Add delta to hidden state `H[:, :-1, :]` and optimize using cross-entropy loss.
4. During generation, apply delta to `H[:, -1, :]` at each decoding step.

```python
# Train delta (step=3)
delta, loss_log = train_delta_from_H(model, tokenizer, prompt, step=3)

# Generate with delta
output = generate_with_delta(model, tokenizer, prompt, delta, max_new_tokens=200)
```

---

## ðŸ”¬ Experiment: With vs. Without Delta

| Setting            | Generation Output (excerpt)                                                           |
| ------------------ | ------------------------------------------------------------------------------------- |
| âŒ No delta         | "The company's strategy remained vague and generic..."                                |
| âœ… delta (3 steps)  | "The company's strategy emphasizes sustainable growth and clean energy..."            |
| âœ… delta (10 steps) | "The new policy outlines a roadmap for carbon neutrality and green infrastructure..." |

---

## ðŸ“‚ Project Structure

```
SLOT-Qwen/
â”œâ”€â”€ delta.pt                  # ä¿å­˜çš„ delta å‘é‡ï¼ˆå¼•å¯¼æç¤ºç”¨ï¼‰
â”œâ”€â”€ LICENSE                   # å¼€æºåè®®
â”œâ”€â”€ preprocess.py             # æ—¥å¿—é¢„å¤„ç†ä¸Ž delta æž„é€ è„šæœ¬
â”œâ”€â”€ README.md                 # é¡¹ç›®è¯´æ˜Žæ–‡æ¡£
â”œâ”€â”€ requirements.txt          # Python ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ SLOT-Qwen3_final.html     # Jupyter Notebook çš„ HTML å¯¼å‡ºç‰ˆæœ¬ï¼ˆç”¨äºŽå±•ç¤ºï¼‰
â”œâ”€â”€ SLOT-Qwen3_final.ipynb    # ä¸»å®žéªŒ Notebookï¼ŒåŒ…å«å®Œæ•´å®žéªŒæµç¨‹
â”œâ”€â”€ SLOT_Paper.pdf            # SLOT åŽŸå§‹è®ºæ–‡ï¼ˆå‚è€ƒç”¨ï¼‰

```

---

## ðŸ“„ Reference

This implementation is based on the following paper:

> **SLOT: Sample-specific Language Model Optimization at Test-time**
> Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi
> \[arXiv Link (TBD)]
> Affiliations: Westlake University, University of Washington, USTC

---

## ðŸ“„ License

This project is licensed under the MIT License.

---

## ðŸ‘¤ Author

This implementation is maintained by Republic, building on the original SLOT idea with integration into the Qwen3-0.6B model.



