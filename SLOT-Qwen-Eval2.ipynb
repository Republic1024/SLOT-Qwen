{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"D:\\pythonProject\\DeepSeek\\Recsys\\AnimeLLMRec\\Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from delta_trainer import train_delta_from_H, generate_by_H, evaluate_slot_ceval, evaluate_slot_ceval_eos, \\\n",
    "    evaluate_slot_ceval_eos_2\n",
    "\n",
    "# 构造 prompt & 得到 H_state\n",
    "prompt = \"请写一段关于AI教育的引言。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "H = outputs.hidden_states[-1]\n",
    "\n",
    "# 调用 delta 训练\n",
    "delta_3 = train_delta_from_H(model, tokenizer, prompt, H, step=3)\n",
    "delta_10 = train_delta_from_H(model, tokenizer, prompt, H, step=10)\n",
    "delta_30 = train_delta_from_H(model, tokenizer, prompt, H, step=30)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc16caa5c81a8ab2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# generate_by_H(model=model, prompt=prompt, tokenizer=tokenizer, delta=delta_3, answer_len=200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86f37fe68c63257",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "# 获取本地路径 \"./ceval-exam\" 中可用的所有子数据集名称（config names）\n",
    "dataset_path = \"./ceval-exam\"\n",
    "dataset_names = get_dataset_config_names(path=dataset_path)\n",
    "dataset_names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9ac52d60c83894",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(r\"./ceval-exam\", name=\"computer_network\")\n",
    "print(dataset['val'][0])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_slot_ceval_eos(model, tokenizer, delta, example, max_len=20, verbose=True):\n",
    "    \"\"\"\n",
    "    基于 generate_by_H_eos 的评估函数，用于 C-Eval 单选题目。\n",
    "\n",
    "    返回：\n",
    "    - predict_option: 预测选项，如 'A'\n",
    "    - is_correct: 是否预测正确\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "\n",
    "题目：{example['question']}\n",
    "\n",
    "选项：\n",
    "A. {example['A']}\n",
    "B. {example['B']}\n",
    "C. {example['C']}\n",
    "D. {example['D']}\n",
    "\n",
    "答案是：\"\"\"\n",
    "\n",
    "    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"🔍 模型生成结果:\\n\", output_text)\n",
    "\n",
    "    predict_option = None\n",
    "    for option in ['A', 'B', 'C', 'D']:\n",
    "        if option in output_text:\n",
    "            predict_option = option\n",
    "            break\n",
    "\n",
    "    is_correct = (predict_option == example['answer'])\n",
    "    # return predict_option, is_correct\n",
    "    return output_text, predict_option, example['answer'], is_correct"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T19:09:04.477686400Z",
     "start_time": "2025-06-05T19:09:04.460688300Z"
    }
   },
   "id": "154970daa54d317c",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_name = \"computer_network\"\n",
    "dataset = load_dataset(r\"./ceval-exam\", name=dataset_name)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "answer_sheet = []\n",
    "for ex in tqdm(dataset['val'], desc=\"Evaluating per-question delta\"):\n",
    "    # === 构造每道题的 Prompt ===\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目，结合题目的知识背景，选择最合适的选项。\n",
    "    题目：{ex['question']}\n",
    "    \n",
    "    选项：\n",
    "    A. {ex['A']}\n",
    "    B. {ex['B']}\n",
    "    C. {ex['C']}\n",
    "    D. {ex['D']}\n",
    "    \n",
    "    答案是：\"\"\"\n",
    "\n",
    "    # === 获取 H_state ===\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    H = outputs.hidden_states[-1]\n",
    "\n",
    "    # === 训练 delta（例如3步）===\n",
    "    delta = train_delta_from_H(model, tokenizer, prompt, H, step=30)\n",
    "\n",
    "    # === 推理与评估 ===\n",
    "    pred, pre_answer, answer, is_correct = evaluate_slot_ceval_eos(model, tokenizer, delta, ex, max_len=20,\n",
    "                                                                   verbose=False)\n",
    "    correct += int(is_correct)\n",
    "    total += 1\n",
    "    answer_sheet.append([pred, pre_answer, answer, is_correct, dataset_name])\n",
    "print(f\"🎯 Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52ec9dd491c117a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_slot_ceval_eos(model, tokenizer, delta, example, prompt, max_len=20, verbose=True):\n",
    "    \"\"\"\n",
    "    基于 generate_by_H_eos 的评估函数，用于 C-Eval 单选题目。\n",
    "\n",
    "    返回：\n",
    "    - predict_option: 预测选项，如 'A'\n",
    "    - is_correct: 是否预测正确\n",
    "    # \"\"\"\n",
    "\n",
    "    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"🔍 模型生成结果:\\n\", output_text)\n",
    "\n",
    "    predict_option = None\n",
    "    for option in ['A', 'B', 'C', 'D']:\n",
    "        if option in output_text:\n",
    "            predict_option = option\n",
    "            break\n",
    "\n",
    "    is_correct = (predict_option == example['answer'])\n",
    "    # return predict_option, is_correct\n",
    "    return output_text, predict_option, example['answer'], is_correct\n",
    "\n",
    "\n",
    "def eval_dataset(dataset_name, step=3, max_len=50):\n",
    "    # dataset_name = \"computer_network\"\n",
    "    dataset = load_dataset(r\"./ceval-exam\", name=dataset_name)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    answer_sheet = []\n",
    "    for ex in tqdm(dataset['val'], desc=\"Evaluating per-question delta\"):\n",
    "        # === 构造每道题的 Prompt ===\n",
    "        prompt = f\"\"\"以下是一道单项选择题，请你阅读题目，选择最合适的选项。\n",
    "        题目：{ex['question']}\n",
    "        选项：\n",
    "        A. {ex['A']}\n",
    "        B. {ex['B']}\n",
    "        C. {ex['C']}\n",
    "        D. {ex['D']}\n",
    "        答案是：\"\"\"\n",
    "        # === 获取 H_state ===\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        # === 训练 delta（例如3步）===\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step)\n",
    "\n",
    "        # === 推理与评估 ===\n",
    "        pred_txt, pre_answer, answer, is_correct = evaluate_slot_ceval_eos(model=model, tokenizer=tokenizer, delta=delta,\n",
    "                                                                       example=ex, max_len=max_len, prompt=prompt,\n",
    "                                                                       verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        answer_sheet.append([ex['question'], pred_txt, pre_answer, answer, is_correct, dataset_name])\n",
    "    print(f\"🎯 {dataset_name} Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return answer_sheet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-05T19:13:58.449239600Z",
     "start_time": "2025-06-05T19:13:58.429240Z"
    }
   },
   "id": "24d29f838758eb6",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(dataset_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1a41f8865f96fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 0\n",
    "max_len = 50\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step, max_len=max_len)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df5bc7122f2a604"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 3\n",
    "max_len = 50\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step, max_len=max_len)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7407072102a5ba5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 10\n",
    "max_len = 50\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step, max_len=max_len)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4fc6a51f4843094",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 5\n",
    "max_len = 50\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step, max_len=max_len)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abcc092b42c4f732",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 6\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6b66e6f93db84a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 30\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17e0a2ecba0b79d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "step = 0\n",
    "answer_sheet = []\n",
    "for i in tqdm(dataset_names[:]):\n",
    "    answer_sheet += eval_dataset(i, step=step)\n",
    "    df_answer = pd.DataFrame(answer_sheet)\n",
    "    df_answer.to_csv(f\"./eval_result/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a145475f7d9618",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b73b50d1b3f57419",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2795484e22285b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for ex in tqdm(dataset['test'], desc=\"Evaluating per-question delta\"):\n",
    "    # === 构造每道题的 Prompt ===\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "\n",
    "题目：{ex['question']}\n",
    "\n",
    "选项：\n",
    "A. {ex['A']}\n",
    "B. {ex['B']}\n",
    "C. {ex['C']}\n",
    "D. {ex['D']}\n",
    "\n",
    "答案是：\"\"\"\n",
    "\n",
    "    # === 获取 H_state ===\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    H = outputs.hidden_states[-1]\n",
    "\n",
    "    # === 训练 delta（例如3步）===\n",
    "    delta = train_delta_from_H(model, tokenizer, prompt, H, step=3)\n",
    "\n",
    "    # === 推理与评估 ===\n",
    "    pred, is_correct = evaluate_slot_ceval_eos(model, tokenizer, delta, ex, max_len=200, verbose=False)\n",
    "    print(pred, ex)\n",
    "    correct += int(is_correct)\n",
    "    total += 1\n",
    "\n",
    "print(f\"🎯 Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe6e22059438a2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_empty_delta(H_state):\n",
    "    hidden_size = H_state.size(-1)\n",
    "    delta = nn.Parameter(torch.zeros((1, 1, hidden_size), device=H_state.device, requires_grad=True))\n",
    "\n",
    "    return delta\n",
    "\n",
    "\n",
    "delta_empty = get_empty_delta(H_state=H)\n",
    "delta_empty.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce3f8fac32a9d102",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for ex in tqdm(dataset['test'], desc=\"Evaluating per-question delta\"):\n",
    "    # === 构造每道题的 Prompt ===\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "    \n",
    "    题目：{ex['question']}\n",
    "    \n",
    "    选项：\n",
    "    A. {ex['A']}\n",
    "    B. {ex['B']}\n",
    "    C. {ex['C']}\n",
    "    D. {ex['D']}\n",
    "    \n",
    "    答案是：\"\"\"\n",
    "\n",
    "    # === 推理与评估 ===\n",
    "    pred, is_correct = evaluate_slot_ceval_eos(model, tokenizer, delta_empty, ex, max_len=20, verbose=True)\n",
    "    correct += int(is_correct)\n",
    "    total += 1\n",
    "\n",
    "print(f\"🎯 Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a559f50747db9f70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from delta_trainer import generate_by_H_eos\n",
    "\n",
    "\n",
    "def evaluate_slot_ceval_eos_2(model, tokenizer, delta, example, max_len=20, verbose=True):\n",
    "    \"\"\"\n",
    "    基于 generate_by_H_eos 的评估函数，用于 C-Eval 单选题目。\n",
    "\n",
    "    返回：\n",
    "    - predict_option: 预测选项，如 'A'\n",
    "    - is_correct: 是否预测正确\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "\n",
    "题目：{example['question']}\n",
    "\n",
    "选项：\n",
    "A. {example['A']}\n",
    "B. {example['B']}\n",
    "C. {example['C']}\n",
    "D. {example['D']}\n",
    "\n",
    "答案是：\"\"\"\n",
    "\n",
    "    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"🔍 模型生成结果:\\n\", output_text)\n",
    "\n",
    "    predict_option = None\n",
    "    for option in ['A', 'B', 'C', 'D']:\n",
    "        if option in output_text:\n",
    "            predict_option = option\n",
    "            break\n",
    "\n",
    "    print(predict_option, example['answer'])\n",
    "    is_correct = (predict_option == example['answer'])\n",
    "    return predict_option, predict_option, example['answer']\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7274c61d5f52b83d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for ex in tqdm(dataset['test'], desc=\"Evaluating per-question delta\"):\n",
    "    # === 构造每道题的 Prompt ===\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "    \n",
    "    题目：{ex['question']}\n",
    "    \n",
    "    选项：\n",
    "    A. {ex['A']}\n",
    "    B. {ex['B']}\n",
    "    C. {ex['C']}\n",
    "    D. {ex['D']}\n",
    "    \n",
    "    答案是：\"\"\"\n",
    "\n",
    "    # === 推理与评估 ===\n",
    "    pred, predict, truth = evaluate_slot_ceval_eos_2(model, tokenizer, delta_empty, ex, max_len=20, verbose=False)\n",
    "    # print(predict,truth)\n",
    "    correct += int(predict == truth)\n",
    "    total += 1\n",
    "\n",
    "print(f\"🎯 Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bacbd63589b8d55",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for ex in tqdm(dataset['val'], desc=\"Evaluating per-question delta\"):\n",
    "    # === 构造每道题的 Prompt ===\n",
    "    prompt = f\"\"\"以下是一道单项选择题，请你阅读题目并选择最合适的选项。\n",
    "\n",
    "题目：{ex['question']}\n",
    "\n",
    "选项：\n",
    "A. {ex['A']}\n",
    "B. {ex['B']}\n",
    "C. {ex['C']}\n",
    "D. {ex['D']}\n",
    "\n",
    "答案是：\"\"\"\n",
    "    print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3be944ae2d6143b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
