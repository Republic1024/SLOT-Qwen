{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0216445bd77a79"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pylab import mpl, plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# best font and style settings for notebook \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "mpl.rcParams['font.family'] = 'MiSans'\n",
    "\n",
    "model_path = r\"./Qwen3-0.6B\"  # modify to your Qwen Path\n",
    "# model_path = r\"./Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c40a0c8ce972a1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from delta_trainer import train_delta_from_H, generate_by_H, evaluate_slot_ceval, evaluate_slot_ceval_eos, \\\n",
    "    evaluate_slot_ceval_eos_2\n",
    "\n",
    "# æ„é€  prompt & å¾—åˆ° H_state\n",
    "prompt = \"è¯·å†™ä¸€æ®µå…³äºAIæ•™è‚²çš„å¼•è¨€ã€‚\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "H = outputs.hidden_states[-1]\n",
    "\n",
    "# è°ƒç”¨ delta è®­ç»ƒ\n",
    "delta_3 = train_delta_from_H(model, tokenizer, prompt, H, step=3)\n",
    "delta_10 = train_delta_from_H(model, tokenizer, prompt, H, step=10)\n",
    "delta_30 = train_delta_from_H(model, tokenizer, prompt, H, step=30)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc16caa5c81a8ab2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# generate_by_H(model=model, prompt=prompt, tokenizer=tokenizer, delta=delta_3, answer_len=200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86f37fe68c63257",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "# è·å–æœ¬åœ°è·¯å¾„ \"./ceval-exam\" ä¸­å¯ç”¨çš„æ‰€æœ‰å­æ•°æ®é›†åç§°ï¼ˆconfig namesï¼‰\n",
    "dataset_path = \"./ceval-exam\"\n",
    "dataset_names = get_dataset_config_names(path=dataset_path)\n",
    "dataset_names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9ac52d60c83894",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(r\"./ceval-exam\", name=\"computer_network\")\n",
    "print(dataset['val'][10])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_slot_ceval_eos(model, tokenizer, delta, example, max_len=20, verbose=True):\n",
    "    \"\"\"\n",
    "    åŸºäº generate_by_H_eos çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº C-Eval å•é€‰é¢˜ç›®ã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - predict_option: é¢„æµ‹é€‰é¡¹ï¼Œå¦‚ 'A'\n",
    "    - is_correct: æ˜¯å¦é¢„æµ‹æ­£ç¡®\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"ä»¥ä¸‹æ˜¯ä¸€é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·ä½ é˜…è¯»é¢˜ç›®å¹¶é€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚\n",
    "\n",
    "é¢˜ç›®ï¼š{example['question']}\n",
    "\n",
    "é€‰é¡¹ï¼š\n",
    "A. {example['A']}\n",
    "B. {example['B']}\n",
    "C. {example['C']}\n",
    "D. {example['D']}\n",
    "\n",
    "ç­”æ¡ˆæ˜¯ï¼š\"\"\"\n",
    "\n",
    "    output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\\n\", output_text)\n",
    "\n",
    "    predict_option = None\n",
    "    for option in ['A', 'B', 'C', 'D']:\n",
    "        if option in output_text:\n",
    "            predict_option = option\n",
    "            break\n",
    "\n",
    "    is_correct = (predict_option == example['answer'])\n",
    "    # return predict_option, is_correct\n",
    "    return output_text, predict_option, example['answer'], is_correct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "154970daa54d317c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from delta_trainer import generate_by_H_eos\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=100):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ past_key_values åŠ é€Ÿï¼Œæ”¯æŒ eos æˆªæ–­çš„ H å±‚æ‰°åŠ¨ç”Ÿæˆã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - model: æ”¯æŒ use_cache çš„ decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT ç³»åˆ—ï¼‰\n",
    "    - prompt: è¾“å…¥æ–‡æœ¬\n",
    "    - tokenizer: åˆ†è¯å™¨\n",
    "    - delta: shape=[1, 1, hidden_size] çš„æ‰°åŠ¨å¼ é‡\n",
    "    - answer_len: æœ€å¤šç”Ÿæˆ token æ•°\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - record_txt: è§£ç åçš„æ–‡æœ¬ï¼ˆä¸å« prompt éƒ¨åˆ†ï¼‰\n",
    "    \"\"\"\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]  # [1, L_prompt]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # åˆå§‹åŒ–æ¨ç†ï¼Œç¼“å­˜ key_values\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # é¦–ä¸ªæ‰°åŠ¨ + ç”Ÿæˆ\n",
    "        H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)  # [1, hidden_size]\n",
    "        logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    record = [next_token_id]  # æ”¶é›†ç”Ÿæˆ token\n",
    "\n",
    "    for _ in range(answer_len - 1):  # å·²ç”Ÿæˆ 1 ä¸ªï¼Œæœ€å¤šç”Ÿæˆ answer_len ä¸ª\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=next_token_id,\n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            H_last = outputs.hidden_states[-1][:, -1, :] + delta.squeeze(1)\n",
    "            logits = torch.matmul(H_last, model.lm_head.weight.T)\n",
    "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        record.append(next_token_id)\n",
    "\n",
    "    # æ‹¼æ¥ç”Ÿæˆåºåˆ—ï¼ˆä¸å« promptï¼‰\n",
    "    gen_ids = torch.cat(record, dim=-1)  # [1, T]\n",
    "    record_txt = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return record_txt\n",
    "\n",
    "\n",
    "def evaluate_slot_ceval_eos(model, tokenizer, delta, example, prompt, max_len=20, verbose=True):\n",
    "    \"\"\"\n",
    "    åŸºäº generate_by_H_eos çš„è¯„ä¼°å‡½æ•°ï¼Œç”¨äº C-Eval å•é€‰é¢˜ç›®ã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - predict_option: é¢„æµ‹é€‰é¡¹ï¼Œå¦‚ 'A'\n",
    "    - is_correct: æ˜¯å¦é¢„æµ‹æ­£ç¡®\n",
    "    # \"\"\"\n",
    "\n",
    "    # output_text = generate_by_H_eos(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "    output_text = generate_by_H_eos_fast(model, prompt, tokenizer, delta, answer_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ğŸ” æ¨¡å‹ç”Ÿæˆç»“æœ:\\n\", output_text)\n",
    "\n",
    "    predict_option = None\n",
    "    for option in ['A', 'B', 'C', 'D']:\n",
    "        if option in output_text:\n",
    "            predict_option = option\n",
    "            break\n",
    "\n",
    "    is_correct = (predict_option == example['answer'])\n",
    "    # return predict_option, is_correct\n",
    "    return output_text, predict_option, example['answer'], is_correct\n",
    "\n",
    "\n",
    "def eval_dataset(dataset_name, step=3, max_len=50, lr=1e-2):\n",
    "    # dataset_name = \"computer_network\"\n",
    "    dataset = load_dataset(r\"./ceval-exam\", name=dataset_name)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    answer_sheet = []\n",
    "    for ex in tqdm(dataset['val']):\n",
    "        # === æ„é€ æ¯é“é¢˜çš„ Prompt ===\n",
    "        prompt = f\"\"\"ä»¥ä¸‹æ˜¯ä¸€é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·ä½ é˜…è¯»é¢˜ç›®ï¼Œé€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚\n",
    "        é¢˜ç›®ï¼š{ex['question']}\n",
    "        é€‰é¡¹ï¼š\n",
    "        A. {ex['A']}\n",
    "        B. {ex['B']}\n",
    "        C. {ex['C']}\n",
    "        D. {ex['D']}\n",
    "        ç­”æ¡ˆæ˜¯ï¼š\"\"\"\n",
    "        prompt = f\"\"\"è¯·ä½ æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„è€ƒè¯•åŠ©æ‰‹ï¼Œé˜…è¯»ä¸‹é¢çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶æ ¹æ®å†…å®¹åœ¨å››ä¸ªé€‰é¡¹ä¸­é€‰å‡ºæœ€åˆé€‚çš„ä¸€ä¸ªã€‚\n",
    "        \n",
    "        é¢˜ç›®ï¼š\n",
    "        {ex['question']}\n",
    "        \n",
    "        é€‰é¡¹ï¼š\n",
    "        A. {ex['A']}\n",
    "        B. {ex['B']}\n",
    "        C. {ex['C']}\n",
    "        D. {ex['D']}\n",
    "        \n",
    "        ç­”æ¡ˆæ˜¯ï¼š\"\"\"\n",
    "\n",
    "        #         prompt = f\"\"\"è¯·é˜…è¯»ä»¥ä¸‹å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶ä»å››ä¸ªé€‰é¡¹ä¸­é€‰å‡ºä¸€ä¸ªæœ€åˆé€‚çš„ç­”æ¡ˆã€‚\n",
    "        # \n",
    "        # é¢˜ç›®ï¼š\n",
    "        # {ex['question']}\n",
    "        # \n",
    "        # é€‰é¡¹ï¼š\n",
    "        # A. {ex['A']}\n",
    "        # B. {ex['B']}\n",
    "        # C. {ex['C']}\n",
    "        # D. {ex['D']}\n",
    "        # \n",
    "        # è¯·ç›´æ¥å›ç­”ï¼Œç­”æ¡ˆæ˜¯ï¼š\"\"\"\n",
    "\n",
    "        # === è·å– H_state ===\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        H = outputs.hidden_states[-1]\n",
    "\n",
    "        # === è®­ç»ƒ deltaï¼ˆä¾‹å¦‚3æ­¥ï¼‰===\n",
    "        delta = train_delta_from_H(model, tokenizer, prompt, H, step=step, lr=lr)\n",
    "\n",
    "        # === æ¨ç†ä¸è¯„ä¼° ===\n",
    "        pred_txt, pre_answer, answer, is_correct = evaluate_slot_ceval_eos(model=model, tokenizer=tokenizer,\n",
    "                                                                           delta=delta,\n",
    "                                                                           example=ex, max_len=max_len, prompt=prompt,\n",
    "                                                                           verbose=False)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        answer_sheet.append([prompt, pred_txt, pre_answer, answer, is_correct, dataset_name])\n",
    "    print(f\"ğŸ¯ {dataset_name} Accuracy (per-question delta): {correct}/{total} = {correct / total:.2%}\")\n",
    "    return answer_sheet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24d29f838758eb6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path.split(\"/\")[-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b02b85fdbbff387",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ college_economics Accuracy (per-question delta): 29/55 = 52.73%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d9166278a2349e4a61a66f5a02e535d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ college_physics Accuracy (per-question delta): 11/19 = 57.89%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bedb47adfdc43039187646fcbe56766"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ college_programming Accuracy (per-question delta): 16/37 = 43.24%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6f6432a53794192a6874eac014dc922"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ computer_architecture Accuracy (per-question delta): 8/21 = 38.10%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fa61492c8004439b487d5438954c2c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step in [12, 15, 18]:\n",
    "    max_len = 5\n",
    "    answer_sheet = []\n",
    "    for dataset_name in tqdm(dataset_names[:]):\n",
    "        answer_sheet += eval_dataset(dataset_name=dataset_name, step=step, max_len=max_len, lr=1e-2)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/0_6B/answer_step_{step}.csv\", index=False)\n",
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7fa44f85f90e6fd5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for step in [3, 0, 6, 9]:\n",
    "    max_len = 5\n",
    "    answer_sheet = []\n",
    "    for dataset_name in tqdm(dataset_names[:]):\n",
    "        answer_sheet += eval_dataset(dataset_name=dataset_name, step=step, max_len=max_len, lr=1e-2)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/0_6B/answer_step_{step}.csv\", index=False)\n",
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387be86860d8f430",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for step in [3, 0, 6, 9]:\n",
    "    max_len = 5\n",
    "    answer_sheet = []\n",
    "    for dataset_name in tqdm(dataset_names[:]):\n",
    "        answer_sheet += eval_dataset(dataset_name=dataset_name, step=step, max_len=max_len, lr=1e-2)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/1_7B/answer_step_{step}.csv\", index=False)\n",
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1a41f8865f96fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for step in [15]:\n",
    "    max_len = 5\n",
    "    answer_sheet = []\n",
    "    for dataset_name in tqdm(dataset_names[:]):\n",
    "        answer_sheet += eval_dataset(dataset_name=dataset_name, step=step, max_len=max_len, lr=1e-2)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/1_7B/answer_step_{step}.csv\", index=False)\n",
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36b7dc42d77e04e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for step in [3, 0, 6, 12]:\n",
    "    max_len = 30\n",
    "    answer_sheet = []\n",
    "    for i in tqdm(dataset_names[:]):\n",
    "        answer_sheet += eval_dataset(i, step=step, max_len=max_len, lr=1e-3)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/1_7B/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39220cfac51cfcc2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for step in [3]:\n",
    "    max_len = 50\n",
    "    answer_sheet = []\n",
    "    for i in tqdm(dataset_names[8]):\n",
    "        answer_sheet += eval_dataset(i, step=step, max_len=max_len, lr=1e-3)\n",
    "        df_answer = pd.DataFrame(answer_sheet)\n",
    "        df_answer.to_csv(f\"./eval_result/1_7B/answer_step_{step}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbde9d64a1dae016",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.DataFrame(answer_sheet)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ab4f334f4c6030",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
